[
["index.html", "XAI Stories Preface", " XAI Stories 2020-06-05 Preface This book is the result of a student projects for Interpretable Machine Learning course at the University of Warsaw and the Warsaw University of Technology. Each team has prepared one case study for selected XAI technique. This project is inspired by a fantastic book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich. We used the LIML project as the cornerstone for this repository. The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using github repository. Cover by kozaka93. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References "],
["foreword.html", "Foreword 0.1 Why? 0.2 What? 0.3 How?", " Foreword Author: Przemyslaw Biecek (Warsaw University of Technology and University of Warsaw) 0.1 Why? Machine learning has a number of applications. Very often, however, machine learning predictive models are treated as black boxes which can be automatically trained without worrying about the domain in which they are used. This opaqueness rises many risks that are difficult to foresee during the model building process. Such as the model’s declining performance due to the data drift, poor performance on the out-of-domain problems or unfair biased behaviour learned on historical data. The growing list of examples where black boxes fail spectacularly has led to an increased interest in XAI methods. Such methods allow to x-ray black boxes models for more detailed analysis on the local or global level. According to Gartner Hype Cycle for Emerging Technologies in 2019 Explainable AI is on the verge of Innovation trigger and Peak of inflated expectations. It is a technology with a very high potential, which is talked about a lot in the media and which heats up the imagination as strongly as AI. In the literature there are many articles arguing the need to use XAI methods as well as many ideas for new methods from the XAI family. However, it is much more difficult to find examples of successful implementations of XAI methods that have improved the business. Missing elements are case studies of actual use of XAI methods in machine learning problems. Such case studies would allow a better understanding of what is possible today and what is not possible using XAI methods. 0.2 What? This ebook collects examples of the use of different methods from the XAI family for different real-world predictive problems. In the following chapters, we show example applications of different XAI techniques to problems based on real-world public dataset. These examples are called XAI stories and like every good story, each one has a structure. It starts with a description of the predictive problem, goes on to describe the proposed model or models. The models are x-rays using XAI techniques to finish the chapter with a point. 0.3 How? For XAI stories to be credible they need not only a strong predictive model, but also business validation of the proposed modeling and explanation approach. Each group of students got two mentors from McKinsey’s Data Science department. The mentors, together with the students, searched for strengths and weaknesses of XAI applications in specific problems. TODO: write more about this collaboration "],
["story-compas.html", "Chapter 1 Story COMPAS: recidivism reloaded 1.1 Introduction 1.2 Models 1.3 Explanations 1.4 Summary and conclusions", " Chapter 1 Story COMPAS: recidivism reloaded Authors: Łukasz Grad (University of Warsaw), Katarzyna Koprowska (Warsaw University of Technology), Jakub Kozak (SGH Warsaw School of Economics) Mentors: Michał Miktus (McKinsey) 1.1 Introduction The study concerns the COMPAS algorithm, which stands for Correctional Offender Management Profiling for Alternative Sanctions, created by a for-profit company Northpointe. COMPAS is a widely popular commercial algorithm used by judges and parole officers for scoring a criminal defendant’s likelihood of recidivism. It was designed to help judges identify potentially more dangerous individuals (those with higher scores) and award them with longer sentences. It is easy to notice that COMPAS results may have very serious consequences for the lives of many people in the United States. yet the algorithm is (because of its proprietary nature) still a black-box for the wide audience – meaning that we cannot easily identify which factors did it take into account when classifying an individual as a person with a high or low likelihood of reoffending. It is natural that many questions have arised about the fairness of such an algorithm, especially as the fairness required to be defined in advance. 1.1.1 Previous work One of the first and the most known investigators wanting to validate COMPAS results was ProPublica group. They have provoked a vigorous discussion about the fairness of black-box models with their 2016 study, which attempted to reconstruct COMPAS methodology. They have collected criminal records from Broward County FL for several thousand people, as well as reports about their offenses in a two-year follow-up period. The overall accuracy of ProPublica’s reconstructed model was only 61%, but their main discovery was a racial bias in favour of Caucasian defendants over those of African-American origins. According to the study, black defendants were particularly likely to be falsely flagged as future criminals almost twice as often as white ones, who were also more often mislabeled as low risk. The researchers believed that this disparity cannot be explained by either defendant’s prior crimes, type of crimes, gender or age. The ProPublica study was, however, criticized for its flawed methodology. One of the critics was Northpointe, the creator of the COMPAS algorithm, who defended the accuracy of its test, because the results from ProPublica do not accurately reflect their model. After ProPublica’s publication, confusion and doubts, whether COMPAS should still be relied on, began to appear among researchers, which led several of them to propose their own validations of the algorithm, based on data provided by ProPublica. One of the most reliable work seems to be the 2019 study “The age of secrecy and unfairness in recidivism prediction” by Rudin et al., which verifies the analysis conducted by ProPublica, indicating cases where the results given by COMPAS may be non-intuitive and possible explanations. The authors believe that ProPublica has drawn conclusions from incorrect assumptions and lack of knowledge of all data: for instance, they assumed linearity in age (as stated in official COMPAS documentation from Northpointe), which appeared to be untrue; they also did not have access to the answers from questionnaires given to defendants (also introduced by the Northpointe as predictors for their model), which can be highly correlated with race and shift the outcome. Disproving ProPublica’s study was not, however, the main objective of Rudin et al. They described what they believed was the real problem of COMPAS: its proprietary nature, which, along with over a hundred of (manually-entered) variables collected from questionnaires not accessible to anyone but Northpointe, does not allow to identify data entry errors, data integration errors, missing data and other types of inaccuracies. The researchers even identified some individuals with a rich criminal history and low probability of recidivism given by model, highly suggesting that their scores were based on flawed data. The main conclusion of this analysis was to replace black-box machine learning models by interpretable models, which, as Rudin et al. suggested, can be equally accurate for predicting recidivism. We believe, however, that we are capable of achieving better accuracy results than the aforementioned researchers and that, thanks to the explainability techniques, a trade-off between accuracy and interpretability is no longer unavoidable. 1.2 Models Our goal was to predict the recidivism and violent recidivism in a two-year follow-up period. Therefore, we modeled both problems as binary classification tasks. Data we used for modelling was largely derived from the “The age of unfairness” study dataset combined with several factors extracted from the raw Broward County FL database used by ProPublica for their analysis. In our model, we analyzed data from 5727 subjects from Broward County FL, whose likelihood of recidivism and violent recidivism we were trying to predict. We have used both 29 variables: personal (6) including current age, age at first offence, sex, race, marital status, custody status, criminal involvement (20) consisting of number and types of previous charges and arrests, as well as those leading to COMPAS screening and history of non-compliance (3) concerning behaviour while on probation. 1.2.1 Recidivism In order to find the best model, we have tried different approaches: Extreme Gradient Boosting (XGBoost), LASSO Logistic Regression and Random Forest. We focused on tuning the XGBoost model, as it provides state-of-the-art results on many tabular datasets, and wanted to compare it with a white box logistic regression model along with out-of-the-box Random Forest. Comparison with a linear model can give us insights on whether the relationships between recidivism and given predictors is highly nonlinear or not. On the other hand, comparison with Random Forest model will reveal how important is careful parameter tuning for this task. 1.2.1.1 XGBoost Since the volume of our datasets is not substantial, we performed XGBoost tuning with exhaustive grid search method in a coarse-to-fine manner. With a coarse parameter search we assessed the relative importance of each parameter and narrowed down its potential range of values. Next, with a much finer search we obtained out final sets of parameters. As a metric during parameter tuning we used AUC on the full training set with out-of-fold scores within a 5-fold CV scheme. In our analysis, we used xgboost package in R. Model Tree Number Max Depth Eta Colsample Subsample Min Child Weight Alpha Lambda XGB 12 3 0.3 1 0.8 3 1 1 XGB Violent 19 3 0.3 0.8 0.8 10 1 0 Selected final parameter sets for recidivism and violent recidivism XGBoost models 1.2.1.2 LASSO regression Since we had strong suspicions about collinearity between predictors, we used Logistic Regression with L1 penalty (LASSO) as our linear model of choice. The penalty term, similarily to XGBoost, was tuned with 5-fold CV on the training set. We utilized a well known glmnet package in R to efficiently find optimal penalty terms using the Lasso path. Final models had \\(17\\) and \\(28\\) non-zero weights, for recidivism and violent recidivism models. Below we present the Lasso path for recidivism model. Lasso path for recidivism model. On top we see the number of non-zero weights. We can see the optimal penalty term chosen close to \\(e^{-5}\\). Best model had 17 non-zero weights, although models with less than 10 chosen predictors also perform well 1.2.1.3 Random Forest As for the Random Forest model, we relied on the default implementation in RandomForest R package, without any finetuning. We set the number of trees to 50, seeing that the chosen XGBoost models were also sparse in tree count. Figure above presents ROC curve for three recidivism classifiers. We decidecied to compare our models using a receiver operating characteristic or ROC curve in short. As it turned out, the most accurate algorithm was XGBoost with AUC of 0.72. Figure above presents ROC curve for three violent recidivism classifiers. We did similar calculations for violent recidivism and again the best results gave us XGBoost. In both cases, ROC AUC was 0.72, which is much more than ProPublika score and also significantly more than COMPAS. 1.3 Explanations 1.3.1 Model specific In order to identify the effect of features on prediction we used Permutation Variable Importance. Not surprisingly, the most relevant variable turned out to be number of previous arrests. Another factor that is greatly affecting prediction is age – younger people tend to be much more likely to commit crime again. It is tightly followed by the number of previous misdemeanors, suggesting that people commiting lesser crimes are more likely to do it again. Last of the important characteristics is the age of first offense, indicating that people whose criminal history begins in their teens are more likely to re-offend. Similarly to recidivism modeling, the number of previous arrests and current age are also the most important factors in predicting violent recidivism. The deciding factor is, however, the number of previous violent charges – people committing violent crimes tend to be more likely to proceed to do it in the future. The impact of the age at first offense is lower comparing to the recidivism model. Our model predicted African-Americans to be, on average, more likely to re-offend than individuals of any other race. 1.3.2 Instance specific In order to identify, whether it is not caused by other factors correlated with race, we have performed an instance level analysis using Ceteris Paribus on observations with the most accurate predictions. Neither is our model – but it has been fitted on real world data full of systematic bias, making it unfair towards African-Americans,as Ceteris Paribus freezes all the other factors. It is very surprising that the COMPAS itself seems to (for some of the subjects) favorite black people over other races, as it can be seen on the picture. Another question arises whether we do not have a gender bias, because both models - ours and COMPAS - seem to drastically change their predictions for male subjects when controlling for other factors. In our next steps we would like to investigate other factors affecting predictions and check whether biases other than racial occur as well. 1.4 Summary and conclusions One may ask: what is the big deal? If data shows that certain individuals are more likely to be classified as reoffending, then our models should include that information. We do not know, however, how existing racial bias influenced the actual data: what if white people were simply less often arrested, had fewer charges and more let go with a warning? This scenario is not so difficult to imagine. As data scientists it is our social duty to make sure we do not propagate any existing biases and do our best to eliminate them. "],
["story-house-sale-prices.html", "Chapter 2 Story House Sale Prices: eXplainable predictions for house sale 2.1 Introduction 2.2 Data 2.3 Model 2.4 Explanations 2.5 Use case of the model 2.6 Summary and conclusions", " Chapter 2 Story House Sale Prices: eXplainable predictions for house sale Authors: Piotr Grązka (SGH Warsaw School of Economics), Anna Kozak (Warsaw University of Technology), Paweł Wicherek (Warsaw University of Technology) Mentors: Mateusz Zawisza (McKinsey &amp; Company), Adam Zmaczyński (McKinsey &amp; Company) ‘’That’s all your house is: it’s a place to keep your stuff while you go out and get more stuff.’’ George Carlin Take-away messages 2.1 Introduction Everybody needs a roof over their heads. It can be a house, a villa, or a flat. Everybody, at some point in life, faces a choice whether to buy a house, and if so, which one. And why are they so expensive? The topic of real estate is not only the topic you just have to deal with. It can also be very interesting. There are plenty of TV Shows, for instance, Property Brothers, of which plot is based on examples of people buying and renovating houses. This particular one is the most famous in the world and has been running already for almost a decade. For many people houses are also an investment that generates profits. Regardless of motives of buying and selling real estate, both sides agree on a price. It is always good to know how much a house is worth, what is the expected transaction price. Furthermore, it may be even more important why is the price like that, what has an impact on it. In this work, we want to find an answer to both questions with a stronger emphasis on the latter. This paper intends to be a comprehensive use case of how to deal with a regression problem for Data Scientists. Let us start with a couple of questions that allow to define and understand problems regarding house pricing. The seller does not know how to increase the value of the apartment so that the investment outlay is lower than the added value (e.g. building a pool may increase the price and renovating the bathroom is not worth it). The seller does not know how much to sell the apartment for (he makes an offer on the portal and does not know if the price is adequate). The buyer does not know how much the apartment is worth (as above, whether the price is adequate). Commercial problem: auction services may be interested in tools to support sellers and buyers (to highlight the sections in the offers that most affect the price). These are just some of the questions we can ask. As a definition of our problem, we set the property valuation, and through explanations we try to get an answer depending on the position we choose. The structure of this paper is as follows. In chapter 1 we introduce the problem of sale house prediction. Chapter 2 shows original data, transformation of variables and external data. Modelling can be found in chapter 3. In chapter 4 we present global and local explanations. Chapter 5 includes a use case for sellers. Chapter 6 summarizes the work. The diagram (Figure 2.1) presents how the research is organized. FIGURE 2.1: Roadmap of the analyses carried out. We started our work with a literature review. The problem of house pricing is typically explained on a basis of hedonic demand theory. It states that each characteristic of a house (such as size or location) has some contribution in its price. Since our data consists of different features of houses, it fits the hedonic theory. Over the years, the problem of property evaluation was solved in many different ways. Statistical tools used by analysts to explain house prices range from simple linear regression to more complex techniques such as artificial neural networks. In the literature we can distinguish two trends, these are publications describing linear models compared to advanced machine learning algorithms (Din, Hoesli, and Bender 2001) and (Selim 2009). In the (Selim 2009) research, can see an artificial neural network prediction errors as compared to linear regression. We can conclude from Figure 2.2 that we reduce the interpretability to an increase in the quality of model fitting. The second trend in articles is the contribution of machine learning to price prediction (Conway 2018) and (Park and Bae 2015). The authors often consider variables that describe the location of a property as an important element of pricing modeling. These include relationships such as postal codes, distances from public transport (bus stops, subways), parks, or cultural sites (Law 2017) and (Heyman and Sommervoll 2019). FIGURE 2.2: Comparison of performance of linear regression model and ANN (artificial neural networks). The models forecast with a logarithm from the property price. On the x-axis we have the prediction measures, and on the y-axis we have the value of this measure on the set divided into linear model and ANN. We can conclude from Figure 2.2 that in referred research it was possible to reduce interpretability in order to increase the quality of model fitting. The next point was data analysis. We work on a dataset which contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015. Data available on Kaggle and OpenML. 2.2 Data We analyzed the data, more. Data contains 19 house features plus the price and the id columns, along with 21613 observations. Below the Table 2.1 describes the variables in the data set. TABLE 2.1: Description of variables in the dataset. Variable Description id unique ID for each house sold date date of the house sale price price of each house sold bedrooms number of bedrooms bathrooms number of bathrooms, where .5 accounts for a room with a toilet but no shower sqft_living square footage of the apartments interior living space sqft_lot square footage of the land space floors number of floors waterfront apartment was overlooking the waterfront or not view how good the view of the property was condition condition of the apartment grade level of construction and design sqft_above the square footage of the interior housing space that is above ground level sqft_basement the square footage of the interior housing space that is below ground level yr_built the year the house was initially built yr_renovated the year of the house’s last renovation zipcode zipcode area lat lattitude long longitude sqft_living15 the square footage of interior housing living space for the nearest 15 neighbors sqft_lot15 the square footage of the land lots of the nearest 15 neighbors Another idea of how to enrich our solution was to add external data. Our hypothesis is that the location of the property can significantly affect the price. We therefore also took into account the distance from public transport and the number of cultural facilities within a kilometer radius. 2.2.1 Data preperation Original data from kaggle is in good quality at the start, but we needed to preprocess it to suit our needs. In this section we describe how we prepared ready-to-use text files of the train and test data (train.csv, test.csv). This includes variable transformations, joining external data, records processing. Firstly, we discovered that there are houses that were sold twice, and one house that was sold three times. One can assume that it was bought, renovated and then sold for more. But they have the same explanatory variables, without changes. Each pair (or triple) representing the same house we decide to aggregate into a single row, averaging the price. 2.2.2 External data We decided to add external data. We believe most variables describe the house properly. What we were missing is some spatial information. Except for zip code, we have information about longitude and latitude, but we wanted to explain it in more detail. Why do some locations tend to be more expensive? Maybe it is because of public transport availability. That is why we decided to add a variable describing a distance to the nearest bus or subway stop. Data sources can be found here. There might also be another reason. Maybe some houses are more expensive, because of some interesting places around, like museums, galleries or fine restaurants. Let us call them cultural places. Those places are not only standalone facilities, but they are connected to other infrastructure, which generally should increase the price. This time we decided to look in the neighborhood: we searched for a number of such places in an arbitrarily chosen 1km range. Data was obtained from here. Both of these external data we visualize in Figure 2.3. Stops are shown on the left and cultural places on the right. Each point indicates the location of the property (marked in green). The blue and red points respectively indicate the location of public transport stops and cultural sites. There are 7549 bus stops and 1214 cultural places in this data. FIGURE 2.3: Spatial external data. Stops on the left and cultural places on the right. Each point indicates the location of the property (marked in green). The blue and red points respectively indicate the location of public transport stops and cultural sites. On the x-axis we have longitude values, while on the y-axis we have latitude values. Most of the cultural places are located in the city center. So this column also tells some story how much of the city center does this house have. Since not all of those places are in the city center, then those other points should reflect some local centers. Public transport stops are very dense in the city center, so they even cover house dots on the plot. Outside of the city center, one can also notice bus routes. It is quite clear that not every house has a good connection to public transport. That can, for example, force parents to drive their children where they need to. For some people it can be an obstacle, so they rather be more skeptical about that particular house. We also did several variable transformations. Since the authors use metric system, we changed square feet into square meters. Zip code and waterfront are saved as factors. Further, it is easier for us to interpret the age of a building rather than a year when it was built. We also know if and when a house was renovated. We can also analyze relative age, that is the time which has passed since the last renovation (variable since_renovated). We chose natural logarithm of price as target variable, since we want to work on a relative scale - we assume that the random error is multiplicative. The last step was dividing the data into train and test samples randomly with a ratio 70/30. Ready script for processing the original data from kaggle can be found on GitHub. For spatial data analysis, we used a short script geofast. Distances between two arbitrary points on earth can be obtained from the geopy (Esmukov 2020) package for Python. However, general and precise formula is computationally expensive. With a simple trick, it can be adjusted to our case without losing almost any precision. This can be done by providing distances to measure are not exceeding several hundred kilometers. The original idea was published here. 2.3 Model Based on the literature we decided to test linear regression models and machine learning models. Below is a list of models that we are considering. linear regression fixed effects model mixed effects model decision tree random forest gradient boosting xgboost We collected methods to evaluate the performance of the regression model and decided to use RMSE (root mean square for our models’ assessment). 2.3.1 Linear models Inspired by literature, we started our analysis by building a white-box linear model. The aim was not to build an ideal model, but rather to get an insight into the relationships in the data and to have a point of reference for more complex models performance and interpretations. Based on economic interpretation, we chose the natural logarithm of price as the dependent variable. The continuous variables characterizing areas were also transformed with natural logarithm. Variables referring to latitude and longitude were omitted since they are unlikely to have a linear effect on price. Instead, zip codes were used to model geographical effects. To avoid collinearity with years since renovation variable, age variable was omitted. 2.3.1.1 Log-log linear model Three versions of the model were estimated. The first model ignores information about house location almost completely (part of this information is carried only by variables such as distance to the nearest bus stop). It assumes that the price of a house in the city center is driven by the same effects as the price of a house in the suburbs. One could argue that this assumption is almost never true, and instead, the data should be modeled by the panel or spatial techniques. 2.3.1.2 Fixed effects model Taking this into consideration, we developed a second model that allows for prices of houses belonging in different zip codes to have their own intercept. The underlying assumption is that the closest area of houses will have a fixed effect on their price. It is the same model as before but with new binary variables (in number equal to a number of zip codes - in our case: 70) indicating whether a house belongs to a certain zip code or not. 2.3.1.3 Mixed effects model A neighborhood, however, can also have an impact on the effect of the particular variables on price. Increasing an area of a house can have a different impact on its price, depending on whether the house is located in the city centre, or in the suburbs. The final model takes this argument into consideration, allowing for houses in different zip codes to have different slopes coefficients for certain variables. Technically, those differences are modeled as random deviations from the general effects. The model was estimated using lme4 (Bates, Maechler, and Bolker 2020) package in R software. The choice of variables that include random effect was made arbitrarily, to reduce the complexity of the problem. Only variables for which a random deviation of their impact could be easily interpreted were chosen (e. g. view importance can vary between geographical locations, since the views themselves are different). We are aware that one could find different approaches that could better fit the data. However, since our scope was not to maximize fit, and no further analysis yielded considerably different results, we decided to limit the consideration to the three models presented. 2.3.2 Linear models assessment Estimated coefficients are presented in the Table 2.2. All of the estimates (apart from m_2_lot_log in first model) are statistically significant on at least 1% level.Since the dependent variable is the logarithm of price, the coefficients have an interpretation of a percentage change. For variables intercept to since_renovated a 1 unit change results in a coefficient *100% change of dependent variable. For variables that were logarithmized (m_2_lot_log to m_2_lot_15_log) a 1% change results in a coefficient % change of dependent variable. Looking at the estimates, a couple of general observations can be made. Firstly, there are no major differences in the coefficients between models, especially between (2) and (3). This tells us that the models can be considered stable. Basing on RMSE, we should note a significant improvement of fit resulting from introducing zip code-based fixed effects in the model (2). A slight, but noticeable improvement was also made by allowing for coefficients to have a random effects in the model (3). These observations suggest that our hypothesis about neighborhood having a significant impact on a house price, but also about particular variables’ effect on price, was reasonable. TABLE 2.2: Estimated coefficients for linear models. Model Log-loglinear (1) Fixed effects (2) Mixed effects (3) (Intercept) 8.1531 8.8260(RE) bedrooms -0.0304 -0.0096 -0.0079 bathrooms 0.0579 0.0409 0.0379 floors 0.0293 -0.0339 -0.0186 waterfront 0.4605 0.5208 0.4641(RE) view 0.0559 0.0609 0.0543(RE) condition 0.0495 0.0557 0.0586 grade 0.1886 0.0885 0.0876 dist_stop_km -0.0106 -0.0041 -0.0047 ncult 0.0134 0.0016 0.0026 since_renovated 0.0038 -0.0002 -0.0007 m_2_lot_log 0.0043 0.0727 0.0841(RE) m_2_above_log 0.3257 0.4099 0.3844(RE) m_2_basement_log 0.0446 0.0270 0.0271 m_2_living_15_log 0.2877 0.1729 0.1566(RE) m_2_lot_15_log -0.0314 -0.0126 (RE) RMSE(train) 0.3054 0.1797 0.1707 RMSE(test) 0.3061 0.1801 0.1762 Estimated coefficients can be interpreted rather intuitively. It is not surprising that the area of the house has a considerable positive impact on price. The same can be said about variables characterizing the quality of the property (grade, condition) and its surroundings (view, waterfront). Especially we can note how important access to the waterfront is in Seattle, that is related to the city geography. Since it is more convenient to have more bathrooms in the house, bathrooms coefficient is also positive. Higher distance to bus stop results in a slight decrease in house value, which again is correct with intuitive expectations. The number of floors, which impact is also negative, can be associated with lower convenience. One may be wondering why years passing since the last renovation does not have a great impact on price. It can be explained by the fact that we already include variables characterizing the condition of the building, and with those being constant – years become not much more than number. To further assess the model performance, we may take a look into the residual density (Figure 2.4). Most of the residuals being in (-0,5; 0,5) range means our model makes the biggest error of approximate -40% and +66%. FIGURE 2.4: The distribution of the residuals from the mixed effects (3) model. On the x-axis, we have the rest of the model for the natural logarithm of price. Plotting residuals against the explained variable gives us some more insight into what happened in the model. FIGURE 2.5: Plot the residual for the mixed effect model. The residuals correspond to the difference between the value of the target variable and the prediction from the model. On the x-axis we have the value of the target variable (price logarithm), on the y-axis of the residual, each point corresponds to the observation from the test set. The cyan line indicates local smooths. We can note a lot of points being in the bottom left part of the plot. This tells us that the model overestimated prices of low-valued houses. The residuals cannot be considered normally distributed. The analysis of residuals clearly suggests that there are some other effects in the data that we failed to model with linear regression. These may result from too few variables being taken into consideration, or, what is more, interesting for us in this work – from the relationships in data being complex and nonlinear. In the following sections, we focus on machine learning models that can perform better in complex environments. 2.3.3 Machine Learning models Our main goal, as we mentioned before, is to explain the model of choice. Along self-explanatory linear models included as comparison, we considered models including: decision tree random forest gradient boosting xgboost. As mentioned earlier, the variables zipcode and waterfront were introduced as categorical. Additionally, for the zipcode variable, which has 70 levels, we use one-hot encoding and build models again. The models were built using mlr (Bischl et al. 2016) R package with ranger (Wright and Ziegler 2015), gbm (Greenwell 2019), xgboost (Chen and Guestrin 2016) and rpart (Therneau 2019). For the models the RMSE score was calculated on the training and test set using the R auditor (Gosiewska and Biecek 2019) package.The results are presented in Figure 2.6. The smallest RMSE can be observed for random forest model, xgboost, xgboost with one-hot encoding, and gbm with one-hot encoding. Other models have a greater error. Let us take a look at these four models. FIGURE 2.6: Comparison of model performance by RMSE on training and test set. In the plot the trained models are marked with points, on the x-axis we have RMSE measure for the training set, on the y-axis the RMSE score for the test set, the line stands for RMSE equal on training and test sets. TABLE 2.3: Model results on the training and test set. Model RMSE train RMSE test random forest 0.0793 0.1675 xgboost 0.1077 0.1600 By analyzing the estimated models we obtained the following results. The random forest model seems to be overfitted, while the xgboost model is more stable. On the test set, they obtained similar scores. Figure 2.7 shows the density of the residuals, for both models. We see that the xgboost model has greater residuals than the random forest model. FIGURE 2.7: Density of residuals for random forest and xgboost models. On the x-axis, we have the rest of the model for the price without transformation. Comparing the xgboost model with one-hot encoding and gbm with one-hot encoding we see that the RMSE score on the training and test set is very similar. Furthermore, the residual density plot (Figure 2.8 ) is practically identical. TABLE 2.4: Model results on the training and test set with one-hot encoding. Model RMSE train RMSE test xgboost one-hot 0.1208 0.1596 gbm one-hot 0.1303 0.1607 FIGURE 2.8: Density of residuals for gbm and xgboost models. On the x-axis, we have the rest of the model for the price without transformation. 2.4 Explanations In this chapter, wepresent the methods of explainable machine learning models. They allow us to understand the compiled models. Following the plan from Figure 2.1, we continue the analytical approach to the regression problem and show how to apply XAI methods in model evaluation. We want to understand how these models work. At the beginning we start explaining four of them, dropping some along the way. This analysis let us derive the explanations for the three groups (seller, buyer, and ad portal) in the next sections. 2.4.1 XAI for geographical location We want to approach explanations by discovering them. As we write this chapter, there are some ideas on how to organize the explainable machine learning workflow, but they are only emerging. We want to explore XAI tools by experimenting with them. We predict that we may face some dead ends on the way, but that is also informative: one can learn from their mistakes. We not only want to show where we are, but also how we got there. We begin our XAI Story by looking at what kind of tools we have. Some are instance-level, some are dataset-level. Some discuss performance, some discuss predictions. Problem with instance-level tools is that they can be applied to every single instance and there are tens of thousands of them in our dataset. Of course we can choose several ones, but the choice is not obvious. Some suggest analyzing those, in case of which the model has the highest residuals (Biecek and Burzykowski 2019). This to work quite nicely as a bottom-up approach. So why not try to explore the top-down approach in this chapter. Start with four models that had the best scores and see what happens. 2.4.2 Feature importance It is highly unlikely that each of explanatory variables used has the same impact on the prediction. We use Feature Importance to evaluate which variables are important in each model. This measure helps the Data Scientists assess which variables have the greatest influence. Below, in Figure 2.9, we present Feature Importance diagrams for four models that have the best RMSE performance. FIGURE 2.9: Comparison of the importance of the permutation variables for four models with the best RMSE score. Colour indicates the model, each bar shows the difference between the loss function for the original data (dashed line) and the permuted data for a particular variable, the longer the bar the greater the difference. The bars for each model start in a different position on the x-axis, this depends on the value of the loss function for the original data set. The comparison shows that the importance of variables differs across the models. It is reasonable since the method of fitting to the data is different for each model. Regardless of the model, however, variables referring to latitude and area of the house seem to have an important impact on the price. This fact suggests that these two variables are the main components of the house price. We can analyze the impact with more detailed techniques. 2.4.3 Partial Dependence Plots (PDP) The models agree on importance of lat and long variables, which are latitude and longitude (basic knowledge might be memorization resistant: longitude describes horizontal (W-E) position, latitude the vertical (N-S) position). In Figure 2.10 we can see that the models for the long variable are compatible, the partial dependence profiles are very similar. For the variable lat, we can see that the random forest model behaves no differently. Analyzing PDP we can state that the prices of apartments located to the north are, in general, higher than those to the south, and properties located to the west have a higher price prediction than those located to the east. FIGURE 2.10: Partial Dependence Profiles for four models, on the left for the variable lat and on the right for the variable long. Each line corresponds to a PDP for one of the four models. The x-axis shows the values of the variable for lat and long, while the y-axis shows the value of price prediction. The second variable worth investigation is m2_living - house surface in square meters. What we expect is a more or less linear increase. The plot seems to confirm this expectation. With grade, we also expected increasing curves, but not necessarily linear. What is interesting, the price goes much higher, if the grade changes from 5-6 to 8-9. We will discuss this phenomenon in detail in chapter 5. FIGURE 2.11: Partial Dependence profiles for m2_living and grade variables. In both cases we see an increase in the value of the prediction along with an increase in the variable. Each line corresponds to one of the four models. On the x-axis we have values of variables corresponding to m2_living and grade. On the y-axis we see the value of property price prediction. Next, we analyze the zipcode variable, which appears to be crucial for random forest model. In this case, to compare all four models, we need a different approach, since two models used the variable encoded as a categorical variable and the other two with one-hot encoding. For models with the zipcode categorical variable, we can see that for the random forest model there are regions for which price prediction is significantly higher than in the others, while for the xgboost model the zipcode variable has no major impact on the price prediction of immovable property Figure 2.12. FIGURE 2.12: PDP for the zipcode variable. At the top, a comparison of the PDP for models with the zipcode variable as categorical. On the x-axis the value of the zipcode variable, and for the y-axis the value of property price prediction. In the lower chart we have a comparison of PDP for gbm and xgboost models with one-hot coding for three selected zipcodes. On the x-axis the value of the variable (0 when the property is in another zip code area, 1 when it is in that zip code area). On the y-axis the value of the real estate price prediction. Let us continue with models that use one-hot encoding of the zipcode variable. We generated all PDP plots and looked at all of the pairs for xgboost_onehot and gbm_onehot for every zipcode. Since there are far too many of them to present, we decided to select only the interesting ones. It turns out that for most of the pairs we observe similar results, but not for all of them. As an example, let us take a look at zipcodes 98010, 98106, and 98018 in Figure 2.12 and 2.13 – we found them on the map. FIGURE 2.13: On the map of Seattle, the zip codes for which PDP was drawn in Figure 2.12 are marked. As we can see, the first two areas are close to the city center, while the last area is far from the center. After a little bit of investigation, victims reveal: in area 98108 there is a loud airport. On the other hand 98010 is far from the city center, there is more space, so people build bigger houses. For instance, in 98010 one can easily find houses with a living area of more than 300m² which is not so common in zipcodes 98106 and 98108. Remember, we were modeling (log of) the price of the whole house, not the price of 1m². There are more examples of variables for which their PDP plots fit our expectations. We can, however, see problems with such variables as age, since_renovated (how many years have passed since a building or last renovation, whichever is less). Figure 2.14 shows comparison of PDP plots for these two variables. The right plot suggests that freshly renovated houses are more expensive, which is reasonable. However, we observe a strange behavior of the random forest model. The left plot may seem surprise: it suggests that the older a house is, the more it costs. This relation can be explained with the fact that older houses tend to be renovated. The rate of renovated houses starts to increase after houses reach 60 years. Half of a century might be a reason to do renovation for sure. To give some numbers: 11% of houses over 60 years old were renovated, and only 0.1% of houses under 40 years were renovated. What is more, since_renovated was more important for all of the models than age. FIGURE 2.14: Partial Dependence profiles for the age variable on the left and for the variable since_renovated on the right. Each line corresponds to one of the four models. On the x-axis there are values of the age and since_renovated variables respectively, while on the y-axis there are values of the property price prediction. For distance to the nearest bus stop PDP plots are shown at Figure 2.15. We trimmed x-axis to 2000 meters, since 92% of observations fulfill that condition. Interesting is, that the shorter distance indicates lower prediction. This is probably due to the fact that larger properties must be located away from public transport stops. FIGURE 2.15: Partial Dependency profiles for the dist_stop variable. Each line is for one of four models. All models behave similarly, the only exception is the random forest model for which PDP is smoother. For properties close to public transport stops (up to 250 meters), the price prediction is slightly lower than for properties at least 250 meters away. 2.4.3.1 PDP summary Most of the PDP plots match our expectations. But there are also variables, where random forest cracks, such as since_renovated, condition, m2_lot, m2_lot15 to mention a couple of them. We guessed that it put far too much attention on the zipcodes rather than other variables, which were incorrectly estimated. That would also explain the huge difference between RMSE on train and test data, Random Forest was overfitted. The other 3 models seem to agree with each other and with so-called common sense. All models are wrong, but Many are usefull, as statisticians say. If three other models agree on the variable effect, it is hard to believe that the other model, which is saying the opposite, is right. At this point, we drop Random Forest and we will not be examining this model from different perspectives. At this point, we also compare two one-hot encoded models, XGBoost and GBM. Their plots are almost identical and they match the XGBoost model without one-hot encoding. So among all 3, we might choose the one with the best score, which is XGBoost. So far we discussed the model-level explanations. The next subsection will focus more on instance-level contemplations. 2.4.4 New possibilities with PDP We went one step forward in the PDP analysis, we looked at the stability of the explanations and a comparison of two types of profiles for evaluating variables in the model. 2.4.4.1 Confidence interval for PDP In the previous subsection, we base our explanations on PDP. A subset of observations is used to construct the PD profile (due to the calculation time). It was interesting for us whether the selection of a subset of observations influences the shape of the PDP. In order to check it, we tested a bootstrap sample size of equal to 100, based on sampling with 500 observations from the test set. Based on the PDP obtained in this way, we constructed a confidence interval, which we calculated as the mean value plus minus standard deviation. In the plots below the results obtained for the 5 variables indicated by the Feature Importance method for the xgboost model. Figure 2.16 shows 100 PDP profiles. As we can see, these profiles are mostly parallel to each other, but there are some values for which they are closer together. FIGURE 2.16: The plot shows 100 PD profiles generated during the bootstrap sample. On the x-axis, we have values of variables and on the y-axis, we have values of prediction. Each line represents one PDP. Based on the PDP obtained, we calculated the confidence intervals. They are illustrated in the 2.17. The blue line is the average of all PDP values at this point, and the grey area is determined, as an approximation of standard deviation. We can now see more precisely that the ranges are not of a constant width for the whole curve. FIGURE 2.17: The plot shows the average PD profile with the confidence interval. The blue line represents the average PDP, while the grey area is the confidence interval. For the x-axis, we have the value of a variable and on the y-axis, we have the value of a prediction. 2.4.4.2 Comparison of Partial Dependence Profiles and Accumulated Local Effects Profiles (ALE) Below we show a list of PDP and ALE . The estimator for ALE profiles eliminates the effect of correlated variables. Since the plot below the PDP and ALE profiles for most of the variables are parallel to each other, it suggests that the model may be additive. To compare the “similarity” of the curves we can use the calculation of the importance of variables based on profile oscillations. For this, we used vivo R package (Kozak and Biecek 2020). FIGURE 2.18: The plots show PDP and ALE profiles for the 5 variables indicated by Feature Importance (Figure 2.9). On the x-axis, we have the values of variables and on the y-axis, we have the value of prediction. The color of the line represents the corresponding profile type. After calculating the importance of variables based on profiles in Figure 2.18, we obtained the following results (Figure 2.19). FIGURE 2.19: A measure based on profile oscillations. The bars indicate the value of the measure and their color corresponds to the profile type. On the x-axis, we have values of the measure and on the y-axis we have variables for which this measure was calculated. We can observe that the values for each variable and profile type do not differ significantly. There is a comparable difference for each variable. This confirms our assumption of no interactions between these variables in the model. An oscillation measure can be used as a variable importance measure for PDP profiles, but also for Ceteris Paribus (CP) profiles to determine the importance of variables for prediction for a single observation. 2.4.5 Instance level explanations. This subsection focuses on explaining particular houses. It is definitely not clear which observations should be examined. In the previous subsection, we examined PDP plots to assess whether we can trust the model and it worked out well. Here we start by looking at: houses with high misprediction and houses with highest average price for 1 square meter. Note, the error is measured linearly on the log-scale, so after the reverse transformation, we look for real estates that had high relative misprediction. What is more, in an earlier analysis, among others we considered zipcodes 98010, a quiet area of Black Diamond with Lake Sawyer. To this list, we add zipcode 98039, which is Medina – mostly residential city. Villas there are vast: 56% of them have at least 300m². The reason is that we want to analyze significantly larger houses in their natural environment. We present the most interesting observations in those zip codes also remembering to cover all of the items on our list above. Let us begin with zipcode 98010. Here we present a property that was the second expensive one. We decided to examine this one, instead of the most expensive one because it had the greater error and the first one we could not locate on Google Street View. This house built 23 years ago is of high grade, has 300 square meters surface, and a large garden of 2000 square meters. Predicted value is 667 k$, while the true value is 902.5 k$. Break Down (BD) plot for this estate is presented in Figure 2.20. FIGURE 2.20: Break Down created with R iBreakDown package for hugely mispredicted house. From the top, a vertical line represents the average response of the model, the green and red bars correspond to the contribution of the variable to the prediction. The green ones take positive values, i.e. increase the prediction value, while the red ones take negative values, i.e. decrease the prediction value. The violet bar corresponds to the prediction value for the observation. The numerical values next to the bars inform about the impact. On the x-axis we have model prediction value, on the y-axis we have variables and their values for the observation. It loses a lot of value because of its geographical position (far from Seattle), but gains a lot on grade and surface. As we can find on a map, this particular property actually has access to the lake and should have been marked with waterfront = 1. If it was, the prediction would be 787325 k$, according to the XGBoost model, and that halves the relative error. From the Figure 2.20 we could also conclude, that houses with waterfront = 0 are not losing so much, but they would highly gain when waterfront would be equal to 1. Another conclusion is that there are mistakes in the data frame. Next case is a house in Medina. We chose the one with the highest average price for 1 square meter of the building. In the Figure 2.21 we see Break Down plot for this house. All factors are in favor of this house. It is in a good location, top grade, living surface is enormous, it has a beautiful view, access to Lake Washington, and the neighborhood is wealthy. Predicted value is 3,961 k$, while the true value was 3,640 k$. Interesting and counterintuitive is that one variable has negative effect, and that is floors. This particular house has 2 levels. In the same picture we also plot Ceteris Paribus (CP) profile for this variable, which says that having two floors is actually the best choice for this observation. FIGURE 2.21: On top Break Down plot for a villa in 98038 zipcode. All variables except floor have positive impact on the price prediction. The second plot represents Ceteris Paribus profile (cyan line) for floor. The dot indicates our observation. The shape of the profile implies that no greater prediction can be obtained for this variable value. 2.5 Use case of the model Having model trained, tested and explained, here comes the time for use cases. First application might be someone who wants to buy a house and improve it to either sell with income, either move in and live in good standard. With model and support of XAI tools there are several ways one can benefit. First approach is that one can study a BD Plot for a particular house and see the loss caused by low grade value. If the influence is highly negative, then we calculate a prediction for the same house, but after renovation: that means with replaced grade value and since_renovated set equal to 0 or 1. That might be very useful in case a buyer has already selected some area of interest and choice between houses is narrowed to a couple of them. In case we only look for houses to renovate and resell with income and specific location is not an issue, then we present a second approach. Here PDP plot can be used. In the Figure 2.22 PDP is monotonically increasing and the greatest change is between grade &lt; 6 and grade &gt; 8. So we can search for houses with such grade, calculate the prediction for modified data and then search for houses with top uplift. We also see that skipping grades 6-8 we also jump from “below average” to “above average”, what can be seen in the histogram attached. FIGURE 2.22: Partial Dependence Profile for grade for xgboost model on left. The blue line corresponds to PDP, for the x-axis, we have the value of a variable and on the y-axis, we have the value of a prediction. On the right histogram of grade, the x-axis indicates the value of the variable, the y-axis a count of observation in each value of grade. We clearly see that prices change drastically when grade goes up from below 6 to above 8. Here we present both approaches in a case study. Using the second method we found for an 85m² house in Kent, WA 98032: 105 year old building, never renovated. 3 bedrooms, 1 bathroom. Grade is 5. Prediction is quite accurate: true value is 176.5 k$ and prediction is 168 k$, so relative error is 5%. Modification described in the previous paragraph results in prediction for 272 k$, 100 k$ more. Of course this predicted change should be compared to renovation cost. In Figure 2.23 there are presented BD plots for this house before and after renovation. FIGURE 2.23: On top Break Down plot for a house potentially good for renovate-and-resell. The impact of the grade variable is negative, the price of the property is decreasing. The second plot, the grade variable have a greater value and since_renovated variable is equal 0. The impact of these variables is positive on prediction. Break Down plot for a house potentially good for renovate-and-resell. When computing the second plot, the same order of variables was used as for the first picture for better comparison. Note that using Ceteris Paribus profile is not exactly correct. Interpretation is that only grade is changed, but renovation also should trigger since_renovation to set to 0. Another application of this model and XAI is at a portal with sell-buy announcements. Portal might be interested in what are the key features that determine the price (where Feature Importance plot can contribute). Then using those important variables, portal can improve searching tools (filters) on the website. Another use for portal is to predict the value based on the features of the house. Then when an user enters an incorrect value (by mistake) it might warn them. Or, on the other hand, if they enter a price higher than predicted, then portal might offer them to promote the offer. 2.6 Summary and conclusions XAI methods are very useful in the work of Data Scientists. They allow us to assess how black box models work. Thanks to this we can compare values obtained between interpreted and complex models. These methods can show what needs to be corrected or improved in the assessment of the global model. The main message from our analysis is the importance of the property’s location and its usable area when setting the property price. It is also worth focusing on local explanations. This allows us to find out what influenced the decision for this particular observation (property). In case of selling the real estate, we can explain what characteristics of houses determine their valuation. Looking at the local explanations the seller can assess how much he can increase the price of his property after a small renovation (we show in our use case). Additionally, in our article we introduce confidence intervals for Partial Dependence profiles, it is a new branch of XAI, we did not encounter in the literature such an approach to check the stability of global explanations based on these profiles. Looking at the whole project in retrospect, the biggest challenge we faced during our work was planning the next steps of analysis, modelling and explanation. At the beginning, we spent a lot of our time on developing an action plan and planning the work. While working on this chapter, we certainly learned to have a business perspective on the analyses we prepared. We owe this knowledge to our mentors, Mateusz and Adam, thank you. References "],
["story-hotel-booking.html", "Chapter 3 Story hotel booking: eXplainable predictions of booking cancellation and guests coming back 3.1 Introduction 3.2 Booking Cancellation 3.3 Repeated guests 3.4 Summary and conclusions", " Chapter 3 Story hotel booking: eXplainable predictions of booking cancellation and guests coming back Authors: Domitrz Witalis (MIM), Seweryn Karolina (MiNI) Mentors: Jakub Tyrek (Data Scientist), Aleksander Pernach (Consultant) 3.1 Introduction The dataset is downloaded from the Kaggle competition website https://www.kaggle.com/jessemostipak/hotel-booking-demand. This dataset contains booking information for a city hotel and a resort hotel in Portugal, and includes information such as when the booking was made, length of stay, the number of adults, children, babies, the number of available parking spaces, chosen meals, price etc. There are 119 390 observations and 32 features. Below you can find features which were used in modelling. Furthermore, feature arrival_weekday was added. Feature Description hotel Resort hotel or city hotel is_canceled Value indicating if the booking was canceled (1) or not (0) lead_time Number of days that elapsed between the reservation and the arrival date arrival_date_month Month of arrival date arrival_date_week_number Week number of year for arrival date stays_in_weekend_nights Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel stays_in_week_nights Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel adults Number of adults children Number of children babies Number of babies meal Type of meal booked is_repeated_guest Value indicating if the booking name was from a repeated guest previous_cancellations Number of previous bookings that were cancelled by the customer prior to the current booking previous_bookings_not_canceled Number of previous bookings not cancelled by the customer prior to the current booking booking_changes Number of changes made to the booking deposit_type Indication on if the customer made a deposit to guarantee the booking. Three categories: No Deposit – no deposit was made; Non Refund – a deposit was made in the value of the total stay cost; Refundable – a deposit was made with a value under the total cost of stay days_in_waiting_list Number of days the booking was in the waiting list before it was confirmed to the customer adr Average Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights required_car_parking_spaces Number of car parking spaces required by the customer total_of_special_requests Number of special requests made by the customer (e.g. twin bed or high floor) market_segment Market segment designation. customer_type Contract - when the booking has an allotment or other type of contract associated to it; Group – when the booking is associated to a group; Transient – when the booking is not part of a group or contract, and is not associated to other transient booking; Transient-party – when the booking is transient, but is associated to at least other transient booking distribution_channel Booking distribution channel. The booking website has information about these reservation characteristics and building models can help this company in better offer management. The most important information could be the prediction of booking cancellation, the prediction if client comes back to the hotel, the prediction whether client orders additional services (eg. meals), customer segmentation. In this project, we have decided to focus on two first issues. 3.2 Booking Cancellation The main aim of this chapter is to build model which predicts whether guest cancels reservation and use some explanation methods to analyze the reasons of customer behavior. 3.2.1 Booking Cancellation: Model In order to predict probability of booking cancellation XGBoost model has been fitted. Table below details the split of dataset. Train Test Number of observations 89542 29848 Number of events 33137 (37%) 11087 (37%) Bayesian optimisation with TPE tuner has been applied in order to improve model performance. Neural Network Intelligence (NNI) package has been chosen for this task, because it provides user-friendly GUI with summary of experiments. List of optimized hyperparameters and chosen values: max_depth - the maximum depth of tree (4), n_esimators - the number of trees (499), learning_rate - boosting learning rate (0.1), colsample_bytree - subsample ratio of columns when constructing each tree (0.78). Figure 3.1 below shows ROC curve of the chosen model. The essential advantages of the model are high AUC and the lack of overfitting. FIGURE 3.1: XGBoost: ROC curve In order to compare blackbox model with an interpretable model a decision tree classifier was trained. It turned out that splits were made by features which are also important in the blackbox model (XGBoost). More details on this are given below. FIGURE 3.2: Decision Tree: ROC curve FIGURE 3.3: “White box” model of booking cancellation: decision tree Let’s take one observation and analyze prediction of two models. We have chosen observation number 187 with features values shown in the table below. Both models predicts high probability of booking cancellation (Decision Tree: 0.9940, Xgboost: 0.9982). We will comment on the readability of the decision tree explanation by plotting the tree later. Feature Value Feature Value hotel 0 arrival_date_month 9 lead_time 321 arrival_date_week_number 36 arrival_date_day_of_month 3 stays_in_weekend_nights 0 adults 2 children 0 babies 0 meal 0 market_segment 4 distribution_chanel 3 is_repeated_guest 1 previous_cancellations 1 previous_booking_not_canceled 0 booking_changes 0 deposit_type 0 days_in_waiting_list 0 customer_type 3 adr 62.8 required_car_parking_spaces 0 total_of_special_requests 0 arrival_weekday 2 FIGURE 3.4: Break down plot explaining prediciton of chosen instance FIGURE 3.5: LIME plot explaining prediciton of chosen instance We can see that explanation of XGBoost model says that features chosen in decision tree have also influence on prediction in XGBoost model. As illustrated in the figure 3.6 if chosen client had not canceled reservation in the past, they would be less likely to cancel this reservation. What is more, if the client had booked hotel later, they would have known their plans better and it would decrease probability of cancellation. Maybe the client canceled booking because of big family event, accident or breaking up with partner (booking for 2 adults). It is impossible to predict those events in advance. FIGURE 3.6: Ceteris Paribus plot explaining prediciton of chosen instance What is the lesson from this example? The performance of the decision tree is worse than XGBoost, so if the explanation of blackbox model is intuitive it is better to use model with higher AUC. 3.2.2 Booking Cancellation: Explanation, dataset level FIGURE 3.7: Feature importance of XGBoost model Figure 3.7 presents the feature importance. The list of five most important features contains deposit_type and previous_cancellations. Intuition suggests that these are important variables in such a problem. There are also variables required_car_parking_spaces, total_of_special_requests, market_segment that will be analyzed later. FIGURE 3.8: Summary of SHAP values of XGBoost model Figure above shows SHAP values. There are some interesting findings which are intuitive: Clients who canceled some reservations in the past are more likely to cancel another reservation. People who buy refundable option cancel reservations more often than others. A lot of days between reservation time and arrival time increases probability of cancelling booking. The longer trip, the higher probability of cancellation. There are also less intuitive findings: Trip personalization (parking spaces, special requests) makes prediction of cancellation be lower. People without any special requests cancel reservation more often than others. If trip starts at the end of the week there is higher probability that customers change their minds. The higher number of adults, the higher probability of cancellation. The probability of cancellation is lower if it is hotel in the city instead of resort hotel. 3.2.3 Booking Cancellation: Explanations, instance level The lowest prediction of cancellation probability FIGURE 3.9: SHAP values and break down plot of XGBoost model for instance with the lowest probability of booking cancellation FIGURE 3.10: SHAP values and break down plot of XGBoost model for instance with the lowest probability of booking cancellation The prediction of probability of cancellation equals 0. The plot of SHAP values shows that client has booked 1 visit and has not canceled it. The values of features previous_cancelations and previous_booking_not_canceled (0 and 1 respectively) make the probability of cancel be lower. The highest prediction of cancellation probability FIGURE 3.11: SHAP values and break down plot of XGBoost model for instance with the highest probability of booking cancellation FIGURE 3.12: SHAP values and break down plot of XGBoost model for instance with the highest probability of booking cancellation The prediction of probability of cancellation equals 1. In the past client canceled one reservation so it is more likely to cancel another one. 440 days between reservation and arrival date makes the probability of resignation be higher. It is intuitive, because the client could have changed plans. Price per night reduces prediction. The value of 75 € per night is cheap compared to the prices in the dataset. We can guess that due to the low price, it may not be important for customers to cancel booking and wait for a refund. 3.3 Repeated guests The main goal of models and explanations in this section is to effectively predict whether the guest will come to the hotel once more, and better understand the factors affecting it. 3.3.1 Repeated guests: Imbalanced dataset The distribution of the answer for the second problem is noticeably imbalanced (the ratio between number of observations with given answer is around 3%). We tested various methods, which are implemented in imbalanced-learn library, in different settings and found the RandomUnderSampler effective and sufficient for our needs as a data balancer for our main model in the second problem and RandomOverSampler as best balancer to use with simple SGDClassifier. The figure below presents the distribution of is_repeated_guest in the dataset. The ratio of this distribution is approximately 30 : 1. FIGURE 3.13: The distribution of guests that are and are not repeated guest in the dataset 3.3.2 Repeated guests: Model This model is meant to predict if the given guest is a repeating guest or not. For this purpose as our main model we chose the XGBClassifier from xgboost package. As mentioned above we have used RandomUnderSampler to balance the training dataset. When explaining various instances with the LIME explainer (the figure below presents the LIME explanation for the best of our first models) for one of the first models we noticed that the model highly relies on previous_bookings_not_canceled and previous_cancellations parameters. We decided to train a model without using those two variables to let the model focus on the other variables. The best models trained without previous_bookings_not_canceled variable had noticeably worse AUC score of 0.9 in comparison to 0.967 AUC achieved by our best models. Because of high influence on the model we decided to keep both variables. FIGURE 3.14: LIME explanation for the best of the first models As a result of the hyper parameter search we have found the optimal set of hyper parameters including: max_depth - the maximum depth of tree (6), n_esimators - the number of trees (100), learning_rate - boosting learning rate (0.33). FIGURE 3.15: ROC curve for the XGBClassifier The model achieved 0.967 AUC, and the figure above presents its ROC curve. We also trained two simpler models - SGDClassifier and DecisionTreeClassifier. While the SGDClassifier (which had the best performance with increased max_iter parameter and when using RandomOverSampler balancer) had significantly worse results than the XGBClassifier, the DecisionTreeClassifier achieved AUC score of 0.94 with the depth bounded by 4. We will focus on the XGBClassifier later, but for the sake of explanation we present the DecisionTreeClassifier tree here. FIGURE 3.16: Plot of the tree of the DecisionTreeClassifier model Unfortunately one can see that the “explainable by design” decision tree model in not easy to understand without usage of model specific methods. While we could use some tree specific explainers, the model agnostic explanations provide quite easy to understand and can be used with a wide range of complex, and often better performing models. 3.3.3 Repeated guests: Explanations, instance level We first inspected the SHAP values for two interesting instances with different correct answer. The first instance showed us that the model learned that guests coming to the hotel in October are less likely to come back and that the lack of booking changes also affects repeating negatively. This explanations are reasonable, because in contrast to the holiday guests, the non-vacation time guests probably are visiting the hotel because of some other, independent reason, that is not as repeatable as the annual vacations. The similar reasoning can be repeated for the changes in the booking and number of special requests - when one comes to some place to relax, they will probably care more about additional attractions provided by the hotel and people who visit a relaxing place, when it met their expectations, probably will come again. The second guest, that is an adult coming to the hotel regularly (previous_booking_not_canceled) for a weekend (arrival_weekday and stays_in_week_nights) probably will come again for one more weekend, for the same reasons as they came before. FIGURE 3.17: Shapley values for a non-repeating customer FIGURE 3.18: Shapley values for a repeating customer 3.3.3.1 Ceteris Paribus plot the same repeating guest From the Ceteris Paribus plot of lead_time variables for the same repeating guest as before we might get even more insight of the model’s reasoning. It clearly shows that the reservation made a year before the visit is an indicator that the guest will more likely come back. It might be a thing that this particular guest has some independent reason to visit the hotel regularly and they knows about it in advance, so because that reason probably is repeating, than they will probably visit the hotel once more. FIGURE 3.19: Ceteris Paribus plot of lead_time for a repeating customer The nonlinearity of the Ceteris Paribus profile of lead_time might be a clue why we were not able to achieve better results with a simple linear model. This result along with more similar ones may lead to effective feature engendering when focusing on creating less complex models. 3.3.4 Repeated guests: Explanations, dataset level The attempt to understand how important are particular variables for the trained model on the dataset level by calculating Permutational Variable Imporatance gave us a clear insight that the previous_booking_not_canceled variable is clearly the most important one, which is very reasonable, because the guest that have visited the hotel before will probably do it once more, in the future. FIGURE 3.20: Permutational variable importance of most important variables for XGBClassifier 3.4 Summary and conclusions Using XAI methods to examine trained models were useful to understand how the trained models work, and see that the explanations are reasonable enough to use the models, along with the explanations, as a great tool for the experts to give them some interesting insights about their customers behaviours. Moreover, even when explaining the complicated models we can get explanations that are easier to read and interpret than easier models, like a single decision tree. Last, but not least, when examining the models we were able to find a dependencies, that might be a partial reason for lower performance of other very simple, but easy to understand, linear model. "],
["story-uplift-modelling.html", "Chapter 4 Story Uplift Modelling: eXplaining colon cancer survival rate after treatment 4.1 Introduction 4.2 Data Preprocessing 4.3 Model 4.4 Explanations 4.5 Summary and Conclusions", " Chapter 4 Story Uplift Modelling: eXplaining colon cancer survival rate after treatment Authors: Aleksandra Łuczak (Warsaw University of Technology), Tymoteusz Makowski (Warsaw University of Technology), Kateryna Shulikova (Warsaw School of Economics) Mentors: Miłosz Dobersztyn (McKinsey), Armin Reinert (McKinsey) 4.1 Introduction We will use data about Chemotherapy for Stage B/C colon cancer from survival package in R. The documentation can be found here, the package can be installed with install.packages(\"survival\") command in R console. After the installation dataset can be accessed via survival::colon command. 4.1.1 What is Uplift Modelling? For classical algorithms in machine learning it is hard to predict causal impact of the event because they are more suited for predicting the results after an action. In some cases, such as a marketing campaign or medical treatment, that causal impact might be extremely important. Due to the possibility of using two training sets (treatment and control groups) by uplift modeling this problem was solved. Uplift modeling is one of the techniques or a branch of machine learning that tries to forecast class probability differences between group exposed to some action or therapy and control group (without that action or therapy). This technique also allows to discover in research those groups of patients for which treatment was most beneficial, so it is commonly used not only in marketing campaigns or medical treatments but also in other customer services. References: Uplift modeling with survival data (Jaroszewicz and Rzepakowski 2014), Uplift modeling for clinical trial data (Jaskowski and Jaroszewicz 2012), Uplift Modeling for Multiple Treatments with Cost Optimization (Zhao and Harinen 2019), Linear regression for uplift modeling (Krzysztof and Jaroszewicz 2018), Ensemble methods for uplift modeling (Sołtys, Jaroszewicz, and Rzepakowski 2015). 4.1.2 Dataset Description We use data from one of the first successful trials of adjuvant chemotherapy for colon cancer. There are two type of treatment: Levamisole is a low-toxicity compound previously used to treat worm infestations in animals and its effects on the treatment of colon cancer have been noted; 5-FluoroUracyl(FU) is a moderately toxic (as these things go) chemotherapy agent. This is the “strongest” one treatment. Both of these medications are given after the cancer excision, it’s adjutant chemistry, that means “extra post-operative”. There are two records per person, one for recurrence and one for death. Dataset contains 1858 observations and 16 features which are described in the 4.1 table. TABLE 4.1: Description of variables found in the dataset with values types from data exploration. Variable Type Description id categorical An id. study categorical 1 for all patients. rx categorical Treatment: Observation, Levamisole or Levamisole+5-FluoroUracyl. sex categorical Patient’s sex: Male or Female. age continuous Patient’s age in years. obstruct binary Stenosis of the colon by the cancer, which is blockage by the tumor. perfor binary Perforation of colon - a flag whether there was a hole in the colon. adhere binary Adherence to the surrounding organs (e.g. bladder). nodes continuous Number of lymph nodes with detectable cancer i.e. during the operation the lymph nodes that were attacked by the cancer are cut out. For the operation to be successful there should be at least 12 lymph nodes. time continuous Days until event or censoring. The time of receiving the treatment is considered to be time = 0, the time that passed in the variable time is the time until death or relapse from receiving the treatment. status binary Censoring status. differ categorical Differentiation of tumour cell (1=well, 2=moderate, 3=poor). The more the better because it is more like colon cells. extent categorical Extent of local spread, what cells did he reach (1=submucosa, 2=muscle, 3=serosa, 4=contiguous structures). The less the better. surg categorical Time from surgery to registration (0=short, 1=long). node4 binary More than 4 positive lymph nodes. etype categorical Event type: 1=recurrence, 2=death. Typically, the survival data includes two variables: the observed survival time (time variable in our dataset) and a binary censoring status variable (status variable in our dataset). The status variable indicates whether the event has been observed (typically denoted by status = 1). However, if the event has not been observed (status = 0) then the true survival time has ben censored, i.e. it is only known that the true survival time is at least equal to time (Jaroszewicz and Rzepakowski 2014). 4.1.3 Ideas There are many ways to use this data: prediction of the patient’s life expectancy depending on whether they received treatment or not, prediction whether the treatment is effective or not, prediction of the life expectancy depending on the medicine administered. We have decided to focus on the last mentioned approach. There are two approaches for modelling this approach. One of them being classification whether the patient will live longer than given threshold (Jaroszewicz and Rzepakowski 2014) and the second one, on whom we will focus, regression which will yield result by how much the treatment will change the life expectancy. 4.1.4 Why is it worth the hassle? When a patient learns about the colon cancer disease they usually ask “How much more time do I have left he has left?”. And now what? What is the treatment? The doctor may indicate a number of therapies that may be effective, but still be unable to tell how much time there is left or what’s the patient’s expectancy to live. The aim of this model is helping to provide more accurate data and answer the patient’s question and how the treatment is going to change their life expectancy. 4.2 Data Preprocessing We removed columns: id, study, etype, study - due to the intent of regression modelling of the problem. The dataset has been divided into: X - all features (without time and rx), y - target variable (time), treatment - rx variable. Distribution of the variable rx is as shown in the 4.2 table. This feature has been categorised. TABLE 4.2: Number of observations with given treatment type. Levamisole Levamisole + 5-FU Observation 620 608 630 4.3 Model To predict model we used algorithms from the package causalml in Python 3. To optimize hyper parameters we used algorithms from the package hyperopt also in Python 3. All notebooks and codes can be found on GitHub. The final model is XGBTRegressor with parameters summarised in the 4.3 table. TABLE 4.3: Final set of parameters used in our model. Parameter Value colsample_bytree 0.8336948571372381 gamma 0.5564260515876811 learning_rate 0.9327196556867555 max_depth 6 min_child_weight 0.45533158266464746 n_estimators 200 This gives as Average Treatment(Lev) Effect \\(74.38\\) and Average Treatment(Lev+5-FU) Effect \\(185.69\\). The average treatment effect (ATE) is a measure used to compare treatments in randomized experiments, evaluation of medical trials. The ATE measures the difference in mean outcomes between units assigned to the treatment and units assigned to the control. In a randomized trial the average treatment effect can be estimated from a sample using a comparison in mean outcomes for treated and untreated units. The treatment effect for individual \\(i\\) is given by \\(y_{1}(i)-y_{0}(i)=\\beta(i)\\). In the general case, there is no reason to expect this effect to be constant across individuals. The average treatment effect is given by the equation (4.1). \\[\\begin{equation} ATE = \\frac{1}{N}\\sum_{i}y_{1}(i)-y_{0}(i) \\tag{4.1} \\end{equation}\\] Where the sum in the (4.1) equation performed over all \\(N\\) individuals in the population. 4.4 Explanations 4.4.1 Dataset Level Explainations 4.4.2 Instance Level Explainations For the instance level explainations we have decided to focus on a limited number of observations. These observations are taken from set which seemed to provide interesting results during the data exploration process. The two observations which we have selected can be found in the 4.4 table. TABLE 4.4: The two observations selected from the dataset for instance level explainations. sex age obstruct perfor adhere nodes status differ extent surg node4 865 0 68 0 0 1 2 1 1 3 0 0 983 1 56 0 0 0 4 0 2 3 0 0 Both patients have the same severity of the cancer which is denoted by the extent variable. One of them is a female and the other is male. The older patient’s (age = 68) cancer is adhered to surrounding organs whilst the other’s cancer is not — this is denoted by the adhere variable. Finally, the last difference is in differ variable which is difference between colon cells and cancer cells. The Explanations plots have been created for both selected observations. For each of the observations there are two different plots. The reason for two different plots is the fact that XGBTRegressor model underneath creates a model for every treatment. Hence we have got one plot for every treatment type there is – in our case Levamisole(Lev) and Levamisole+5-FluoroUracyl(Lev+5FU) – as seen on figures below. 4.4.2.1 Break Down On the figures FIGURE 4.1: Break Down plots for patient with id 865. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. FIGURE 4.2: Break Down plots for patient with id 983. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. 4.4.2.2 LIME FIGURE 4.3: Lime plots for patient with id 865. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. FIGURE 4.4: Lime plots for patient with id 983. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. 4.4.2.3 SHAP The SHAP plots have been created for both selected observations. For each of the observations there are two different plots. The reason for two different plots is the fact that XGBTRegressor model underneath creates a model for every treatment. Hence we have got one plot for every treatment type there is – in our case Lev and Lev+5FU – as seen on figures 4.5 and 4.6. FIGURE 4.5: SHAP plots for patient with id 865. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. FIGURE 4.6: SHAP plots for patient with id 983. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. 4.5 Summary and Conclusions References "],
["story-meps-explainable-predictions-for-healthcare-expenditures.html", "Chapter 5 Story Meps: eXplainable predictions for healthcare expenditures 5.1 Introduction 5.2 Model 5.3 Explanations 5.4 Summary and conclusions", " Chapter 5 Story Meps: eXplainable predictions for healthcare expenditures Authors: Anna Kozioł (Warsaw University of Technology), Katarzyna Lorenc (Warsaw University of Technology), Piotr Podolski (University of Warsaw) Mentors: Maciej Andrzejak (Affiliation 2), Alicja Jośko (Affiliation 1) 5.1 Introduction Perhaps the most urgent problem with the current health care system in the United States is its high cost. According to the Centers for Disease Control and Prevention, during 2017 health care spending per capita averaged nearly $11,000 and total spending was $3.2 trillion, or 17.9% of GDP. This raises the natural question of the causality of high expenses and the estimation of them for a particular person. One of the objectives of this chapter is to forecast annual spending on the health care of individuals in the United States. There is no doubt that these forecasts are of interest to people directly related to medical expenditure, for example, insurance companies, employers, government. How to deal with a situation when the model works well but is a so-called black box and we do not know what affects a specific result? What if the proposed models return non-intuitive results and we want to know why they are wrong? The next and main purpose of this chapter is to address these concerns using Explanatory Model Analysis. We will try to identify not only which features are most predictable for the results, but also the nature of the relationship (e.g. its direction and shape). We will focus on understanding the behavior of the model as a whole, as well as in a specific instant level (for specific person). The data set comes from a study called Medical Expenditure Panel Survey (MEPS), which is sponsored by the Healthcare Quality and Research Agency. About 15,000 households are selected as a new panel of surveyed units, regularly since 1996. Data set used for analysis is available for free on the MEPS website. The MEPS contains a representative sample of the population from the United States with two major components: the Household Component and the Insurance Component. Household Component collects data about demographic characteristics, health conditions, health status, medical history, fees and sources of payment, access to care, satisfaction with care, health insurance coverage, income, and employment for each person surveyed. The second component - insurance - collects data about the health insurance from private and public sector employers. The data include the number and types of private insurance schemes offered, premiums, employers’ and employees’ health insurance contributions, benefits associated with these schemes, and employer characteristics. The data processing and analysis were carried out in Python 3.7.3 and R 3.6.1. 5.2 Model 5.2.1 Data Agency of Healthcare Research and Quality provides an extensive database of medical expenses. Consequently, dataset selection on which we will make further analysis was an important first step. We decided to choose the two latest panels. Expenditures for treatment that we will examine in the following chapter apply to the years 2015/2016 and 2016/2017. The selected dataset contains information on over 32,000 patients, and each of them is described by 3,700 variables. We attached great importance to choosing features that would be appropriate for the prediction. The most important criterion adopted is that the variable cannot relate to expenditure associated with any treatment. For this purpose, we looked through several hundred of them and selected 387 most suitable. As a part of the preprocessing, we removed records that were marked as Inapplicable in the expenditure column. The number of people who didn’t incur expenses is 5504, while the number of patients with “inapplicable” is 407, the percentage respectively are 17% and 1%. The following figures show the distribution of the explained variable. FIGURE 5.1: Distribution of medical expenses 5.2.2 Model Among the models we have trained, the best results were achieved by Gradient Boosting. Due to the characteristics of the explained variable, we decided to check the behavior of the model after applying the logarithmic transformation to expenses. We also checked whether the not inclusion of people without medical expenses would affect the model. Model hyperparameters have been tuned using NNI toolkit. To choose the best model, we compared the determination coefficient values. The table below shows the results of the experiments. To calculate the determination coefficient in column \\(R^2\\) (logarithmically transformed expenses), we transformed logarithmically the values of expenses, and after training the model we returned to the original scale. Values of the determination coefficient. Patients without expenses Model \\(R^2\\) \\(R^2\\) (logarithmically transformed expenses) included Gradient Boosting 0.50 - included Tuned Gradient Boosting 0.55 - not included Gradient Boosting 0.42 0.40 not included Tuned Gradient Boosting 0.49 0.49 The best fit relying on the determination coefficient was demonstrated by a Gradient Boosting, which included patients who did not incur treatment expenses. Then, as a compromise between the size of the model and its quality, we chose the 7 most important variables. For this purpose, we ranked the significance of the variables in the model and extracted those with the highest coefficient. Obtained variables mainly concern the number of visits to specialists. For a more diverse and interesting analysis, we have also taken into account demographic variables such as age, gender, educational background, and race, as well as some disease units. FIGURE 5.2: Scheme of conduct with a specification of origin of variables A review of selected variables Variable Description IPNGTDY1 number of nights associated with hospital discharges OBDRVY1 number of office-based physician visits HHAGDY1 agency home health provider days DSFTNV5 indicate whether the respondent reported having his or her feet checked for sores or irritations OBOTHVY1 office-based non-physican visits PSTATS2 person disposition status OPOTHVY1 outpatient dept non-dr visits AGE2X age of patient RACEV2X race of patient SEX patient’s gender HIDEG the highest degree of education attained at the time the individual entered MEPS diab_disease indicates whether the patient suffered from a diabetes disease art_disease indicates whether the patient suffered from a arthritis disease ast_disease indicates whether the patient suffered from a asthma disease press_disease indicates whether the patient suffered from a high pressure disease heart_disease indicates whether the patient suffered from a heart disease In the following section we will explain the Gradient Boosting model based on 16 variables presented in the table above. The coefficient of determination of the final model is 0.5 TODO: zaktualizowac wartosci R^2 5.3 Explanations 5.3.1 Model Level Explainations In order to find out about the influence of individual variables on the prediction for each patient, we present a Permutation Variable Importances graph. FIGURE 5.3: Permutation Variable Importances for Gradient Boosting Regressor Undoubtedly, the most important is the variable that indicates the number of nights spent in the hospital by the patient. An interesting observation seems to us that the demographic variable - AGE, which initially did not have a significant impact on the prediction, on the reduced model is in the top five most important variables. The remaining demographic variables, as well as those relating to diseases, do not show a gain in relevance in a model reduced to several variables. Based on previous analyzes, the number of nights spent in the hospital turned out to be the most important variable. To understand the nature of its impact on prediction in our model, it’s worth looking at the Partial Dependence Profiles. Below we present the PD plots broken down by gender. FIGURE 5.4: Partial Dependence Profiles for number of nights spent in the hospital broken down by gender Among patients who spent a few nights in the hospital, gender is not important for the amount of prediction. This rule begins to change after exceeding 30 nights. The PD profile for men has significantly higher values compared to the women’s profile, despite the similar curve behavior. After exceeding 70 nights in the hospital, this variable does not affect the result returned by the model on average. 5.3.2 Instance Level Explainations - business approach In this subsection we will try to show the application of explanatory methods in the business approach. Selected observations are: 1) the person with the best estimated cost among people with results greater than 3000, 2) the person for whom the model predicted the highest cost of all. Finding the value of characteristics that increase or decrease the final result, diagnostics of the direction of changes or oscillations of the result in case of change of characteristics describing a person may be valuable information for insurance companies or other payers for medical services. Such conclusions may also be useful for the patients themselves, who have decided to pay for medical care themselves. 5.3.2.1 XIA for the best prediction Prediction of medical costs for first observation is 3882$ and is differed from the real value by 4.8$. 5.3.2.2 XIA for the best prediction using Break Down Plots Break-down plots show how the contribution of individual variables change the average model prediction to the prediction for observation. FIGURE 5.5: Brake down plot The patient has 58 age, which alone increase average total cost by 4303.195 $ and the gender is female which decreases average total cost by 140. 72 $. Her total number of office-based visits is 4, which increase average total cost by 574.177 $. She suffers from arthritis what increase average total cost by 677.868 $ but she is not diagnose to diabets, astma or high blood preasure which decrease final result. Her status of education is unknown, what increase total cost. The fact that she didn’t spend any night in the hospital decrease average total cost by 1343.271$. And also, she didn’t benefit from home medical services decrease average total cost by 386.772 $. 5.3.2.3 XIA for the best prediction using Shapley Values To remove the influence of the random ordering of the variables in brake down results we can compute an average value of the contributions. FIGURE 5.6: Shapley values plot The plot shows that the most important variables, from the point of view of selected observation is age, number of night spending in the hospital and diagnose high blood preasure. For this obserwation having age equals 58 decrease average total cost by 116.435 $. Similar impact (decrease result by 116.25 $ ) have the fact that she didn’t spend any night in the hospital. Her status of education is unknown, what decrease average total cost by 104.521 $. She is not diagnose the high blood preasure which increase average response by 113.134 $. For this obserwation being a women increase average total cost by 70.099 $. 5.3.2.4 XIA for the best prediction using LIME TODO: poprawic wykresy lime The key idea behind this method is to locally approximate a black-box Gradient-Boosting model by a K-lasso interpretable model. FIGURE 5.7: LIME plot According to the LIME method, the plot suggests that spending any night in hospital reduces the estimated cost by $663. Much greater, also the negative impact has a variable which telling that that patient didn’t benefit from home medical services, which total cost by 16534.29 $. Patient analysed is not diagnose the high blood preasure which decrease response by 1308.27 $. Variables that increase the cost of medical services are the total number of office-based visits greater than 3 and the age of the analyzed person greater than 54. 5.3.2.5 XIA for for the best prediction Prediction of medical costs for second observation is 147178.5 $ and is differed from the real value by 3721.5$. 5.3.2.6 XIA for the prediction with the largest cost using Break Down Plots FIGURE 5.8: LIME plot The patient spending 52 nights in hospital, which increase average total cost by 4304.195 $. Having 59 age, increase average total cost by 16986 $ and the fact that gender is male increase average total cost by 15105 $. His status of education is bechelor degree, what increase total cost 1887 $. Despite he is not diagnose to diabets and high blood preasure which decrease final result, he suffers from heart disease which also decrese avarage response. 5.3.2.7 XIA for the prediction with the largest cost using Shapley Values FIGURE 5.9: Shapley Values plot As we expected, for the observation that generated the highest predictions, most of the variables have an additive effect on the final result. The graph shows that the greatest influence on the final value of the average prediction was in being male. Living in a household increases the average prediction by 18031$. The fact of having heart disease, spending 52 nights in hospital and the total number of office-based visits equal to 6 also had a big positive impact on the result. 5.3.2.8 XIA for the prediction with the largest cost using LIME FIGURE 5.10: LIME plot The LIME method also returns a positive influence on the final prediction for most variables. The chart shows that spending 52 nights in hospital increases treatment costs by 32360 $. The total number of office-based visits greater than 3 also has a positive impact, increasing the prediction by 6074.66 $. The age of a patient over 54 also significantly increases medical costs. Among the variables reducing treatment costs ,was the total number of days in home health care equal to 0. 5.3.2.9 XIA for both predictions using Ceteris Paribus Profiles Here we will denote the patient with the largest cost predicted as Patient 2, and the patient with the best prediction as Patient 1. FIGURE 5.11: Ceters Paribus plot for selected 6 features In the plot above we have selected features that behave differently between those two patients. In later subsections we will dive more into those differences. 5.3.2.9.1 Comparing differences between patients based on Sex. FIGURE 5.12: Ceters Paribus plot for SEX As we can see, our explanation model tries to predict what would have happened if the patient would have a different sex And for the patient 1 there would be no difference in predicted value, but for patient 2 the change of sex implicates lowering or increase in predicted costs. 5.3.2.9.2 Comparing differences between patients based on age. FIGURE 5.13: Ceters Paribus plot for AGE In this section we will investigate age. For patient 1 there is no influence on age on the expenses, but for patient 2 there is an influence of this variable. For patient 2 being around age of 60 and below age of 10 implicates a rise in predicted costs. 5.3.2.9.3 Comparing differences between patients based on number of nights associated with hospital discharges. FIGURE 5.14: Ceters Paribus plot for IPNGTDY1 Here both patients behave similary, but with different sensitivity. Patient 2, because of higher costs is more sensitive in changes of the number of nights associated with hospital discharges. 5.3.3 Instance Level Explainations - instance specific approach Here I will try to show the results of explanatory analysis methods for selected instances of data, that is for specifically selected different people with different background, race, age and sex. A review of selected variables Variable Patient 1 Patient 2 IPNGTDY1 0 3 OBDRVY1 12 8 HHAGDY1 0 0 DSFTNV5 2 -1 OBOTHVY1 0 6 PSTATS2 11 11 OPOTHVY1 0 3 AGE2X 71 34 RACEV2X 2 1 SEX 1 2 HIDEG 1 1 diab_disease 1 0 art_disease 1 0 ast_disease 0 0 press_disease 1 0 heart_disease 1 0 real expenses 2263 16268 prediction 8779 24373 So as in table above we will investigate 2 patients, where one is of age 71 and with several illnesses, where the second one is of age of 34, different sex and without illnesses. 5.3.3.1 XIA for Patient 1 using Break Down Plots Here we will be showing explanations using Break-down plots and explain the contribution of individual variables on the prediction, The first patient is of age 71, which in comparison to previous explanations should have significant impact on the prediction but not in this case. Here the biggest influence on the prediction has variable OBDRVY1, which is the number of office-based physician visits. Also diabetes and indicator whether the respondent reported having his or her feet checked for sores or irritations have positive influence on the predicted expenses. But variables IPNGTDY1, OBOTHVY1 and HIDEG have most significantly negative influence on the prediction. 5.3.3.2 XIA for Patient 1 using Shapley Values We can run explanatory analysis using shaplay values for those patients. For Patient 1, all observations apart from age have positive influence on the predicted value. 5.3.3.3 XIA for Patient 1 using LIME TODO: poprawic wykresy lime 5.3.3.4 XIA for Patient 2 using Break Down Plots The second patient is of age 34. In this case the most significant influences on the prediciton have variables IPNGTDY1 and OPOTHVY1 which are responsible for almost 17k of expenses. Other variables that also influence the prediction are OBDRVY1 and OBOTHVY1. Other variables have very small or small but negative influence on the prediciton. #### XIA for Patient 2 using Shapley Values We can run explanatory analysis using shaplay values for those patients. For Patient 1, all observations apart have positive influence on the predicted value. 5.3.3.5 XIA for Patient 2 using LIME TODO: poprawic wykresy lime 5.3.3.6 XIA for Patient 1 and Patient 2 using Ceteris Paribus Profiles In this chapter we will use Ceteris-paribus profiles for instance level explanations. Ceteris-paribus profiles show how the model response would change if a single variable is changed. So here we will be checking how would the model prediction change, if we change only one property of the patient and how it influences our model 5.3.3.6.1 Comparing differences between model predictions for Patient 1 and 2 for diffenret properties Here we present a plot, on which we list all variables that are different between patients and we will try to investigate, how the model behavior changes for each patient, when their properties change. FIGURE 5.15: Ceters Paribus plot for six variables As shown on the plot above, each value of the patient property influences a bit differently the outcome of our model. Patient 2, who has higher expenses is more sensitive on changes of the values of his properties. 5.3.3.6.1.1 Comparing differences between patients for number of nights associated with hospital discharges FIGURE 5.16: Ceters Paribus plot for IPNGTDY1 For this property of our patients, we would like to show how the model responds, when values of the number of nights associated with hospital discharges influences the predicted costs. The number of nights influences costs for patient 1 and 2 similary, but with different size. Patient 2, due to having higher prediction of expenses is being influenced more, than patient 1, but with the same dynamic. 5.3.3.6.1.2 Comparing differences between patients for variable if feet was checked for sores or irritations FIGURE 5.17: Ceters Paribus plot for DSFTNV5 For the variable if of the feet of the patients was checked for sores or irritations only patient 2 shows some responce for any change of the property. 5.3.3.6.1.3 Comparing differences between patients for variable of number of office-based physician visits. FIGURE 5.18: Ceters Paribus plot for OBDRVY1 In this case we can notice an interesting influence of the variable OBDRVY1. For patient 1 the higher the number of office-based physician visits the higher would be the predicted outcome of our model. This is different for patient 2, for whom the predicted value is not changing, sometimes it is even declining. 5.4 Summary and conclusions All key information about the final model we will put in this section. Scripts and list of selected variables are available at: meps_story github. "],
["story-meps-healthcare-expenditures-of-individuals.html", "Chapter 6 Story MEPS: Healthcare expenditures of individuals 6.1 TODOS 6.2 Introduction 6.3 Models 6.4 Explanations 6.5 Summary and conclusions", " Chapter 6 Story MEPS: Healthcare expenditures of individuals Authors: Dominika Bankiewicz (University of Warsaw), Jakub Białek (Warsaw University of Technology), Agata Pładyn (Warsaw University of Technology) Mentors: 6.1 TODOS Opisy wszystkich zmiennych do appendixa (tabela?) Rozkład/percentyle Y, przed i po transformacji (wtedy łatwiej ocenić wyniki absolutne) do wstępu. Podrozdzział z wynikami każdego z modeli? W rozdziale modele - krótki wstęp, tam napisać, o transformacji y i o tym, że korzystaliśmy z tego samego train/test splitu. 6.2 Introduction In this part, models that predict annual healthcare expenditure of individuals will be developed and analyzed using XAI methods. The data set analyzed is called MEPS (Medical Expenditure Panel Survey) and it is freely accessible at [ref1]. The data comes from large-scale surveys of families, individuals, medical providers and employers from the United States of America. Each observation of the data set contains total expenditures of an individual as well as number of other variables describing his or her demographic and socio-economic status. This allows to create models predicting the expenditure based on other factors. For this reason it is particularly interesting from the point of view of the subject that is financially responsible for the healthcare cost – insurance company, government, healthcare provider or individuals. It is important to mention, that for all of these subjects, accurate model is only a part of the success – the other part are the relations between input parameters and predictions of the model. Having that we can provide an answer not only for how much? but also for why? Hopefully, this can be achieved with available XAI methods. The data analyzed in the following sections was not downloaded directly from MEPS website. Instead, it was obtained through IBM’s AIX360 [ref2]. Therefore it is initially preprocessed – race is restricted to black and white and ethnicity to non-Hispanic, records with special values (negative integers) are removed for some of the variables, variables are initially selected. The dataset provides 18350 observations and it contains variables that describe: demographics (age, gender, marital status), socio-economics (education, income, insurance coverage), self-reported health status (self-evaluation of physical and mental health status), diagnoses (stroke, cancer, heart disease, diabetes), health limitations (cognitive, activity, sensory). The following section describes the development of three different models for predicting the transformed total health expenditure of an individual. Once developed, these models are compared in terms of their quality and of them is selected and analyzed using XAI methods. Explanations provided by different methods are discussed. The baseline for this discussion was provided by the authors of IBM’s AIX360 [ref3]. 6.3 Models In the following subchapters the development of three different models is briefly described. Full details on the implementation together with the code itself can be found here. First of all, since the distribution of the predicted variable is strongly skewed, it was transformed with logarithm base 3. Typically in such cases, natural logarithm is chosen, but having in mind that impact of input variables on the prediction will be analyzed, the decision was made to use base 3 instead (when we see that some input variable affected the prediction increasing it by one, then we can say that it increased the total expenditures by the factor of three, not factor of Euler number). In order to ensure that results from all three models can be directly compared, models are trained and evaluated on the same, arbitrarily chosen, test and validation subsets. These can also be found at [repo]. The evaluation metrics are RMSE, MAE and R^2 [ref to the table with results]. 6.3.1 Model 1: Linear Linear model was created with sklearn package. Following operations were performed to prepare the data: logarithm change of the explainable variable (as described before) min and max scaling of variables RTHLTH31, MNHLTH31, POVCAT15. This variables describe the state of general health, mental health and poverty status in values (decoded to: poor, fair, good, very good, excellent). addition of a new column which counts for how many diseases/health issues patient was tested positive. The ridge regression was performed with GridSearchCV. This allowed to perform 5-fold cross-validation with a range of different regularization parameters in order to find the optimal value of alpha parameter. The final results of the ridge regression model are shown in a table (values rounded to 2 decimal points): RMSE MAE R^2 Train 2.27 1.72 0.33 Test 2.25 1.7 0.3 Figure presents the prediction values compare to real values of target variable on training dataset. Figure presents the prediction values compare to real values of target variable on test dataset. 6.3.2 Model 2: ANN The second model evaluated was a multilayer perceptron. The input data was preprocessed with use of scikit-learn [ref to sklearn] tools: numerical features were standardized categorical features were one-hot encoded The data was fed into ANN with 4 hidden, fully-connected layers. The ANN model itself was created with Keras [ref to Keras]. Alltogether, the preprocessing steps and the model, was wrapped into scikit-learn pipeline so it can be easily use with XAI methods. The results are shown in the table below: RMSE MAE R^2 Train 2.04 1.51 0.45 Test 2.17 1.61 0.37 6.3.3 Model 3: XGB XGB model was developed using scikit-learn package (scikit-learn 2019a). For this model, data was prepared in the following way: as was mentioned in Introduction, target variable was transformed with logarithm base 3, categorical features (i.e. features with 10 or less unique values, except variables “POVCAT15”, “RTHLTH31”, “MNHLTH31” which can be treated as continuous) were transformed with OneHotEncoder (scikit-learn 2019c), numerical features were transformed with StandardScaler (scikit-learn 2019d). Hiperparameters tuning was done with GridSearchCV (scikit-learn 2019b). Following parameters were optimized: n_esimators - the number of trees, max_depth - the maximum depth of tree, min_samples_split - the minimum number of samples required to split an internal node, min_samples_leaf - the minimum number of samples required to be at a leaf node. Results for best hyperparameters shows below table. RMSE MAE R^2 Train 2.04 1.51 0.45 Test 2.17 1.61 0.37 Figure presents the prediction values compare to real values of target variable on training dataset. Figure presents the prediction values compare to real values of target variable on test dataset. Finally, for XAI methods analysis, model was wrapped in pipeline. 6.3.4 Results Table below presents results for all developed models. RMSE (train) MAE (train) R^2 (train) RMSE (test) MAE (test) R^2 (test) LR 2.27 1.72 0.33 2.25 1.70 0.30 ANN 2.14 1.59 0.40 2.17 1.61 0.37 XGB 2.04 1.51 0.45 2.17 1.61 0.37 6.4 Explanations 6.4.1 Instance level 6.4.1.1 Observation with the best prediction 6.4.1.2 Observation with the worst prediction This patient it’s a 76 years old woman. She is a widow and has a GED or high school degree. She complains about poor health status and fair mental health status. She has ever been diagnosed with high blood pressure, coronary heart disease, emphysema, chronic bronchitis, high cholesterol, cancer, rheumatoid arthritis and joint pain last 12 months. She has social and cognitive limitation and limitation in physical functioning. She doesn’t smoke and has serious difficulty see or wears glasses. Her health insurance coverage indicator is public only. Total health expenditure of this woman is equal to zero but model predicted that it is equal to about 12005.68 (3 to the power of 8.55). Figure presents Break Down plot for observation selected in this subchapter Picture below shows Shapley Values for this observation. As we can see, the largest impact on total health expenditure of this woman is the fact that she has been diagnosed with cancer, high blood pressure and how old she is. Figure presents Shapley Values for observation selected in this subchapter Three plots below show the Ceteris Paribus profiles for this observation. They say how total health expenditure of this woman would change if a variable had a different value with the other variables unchanged. They complete the chart with Shapley Values. As you can see, the fact that this woman was diagnosed with cancer increases her spending by about 3 to the power of 0.5 times, while the fact that she was diagnosed with high blood pressure increases her expenditure by about 3 to the power of 0.3 times. If she were younger by at least about 10 years, expenses would be smaller, and their would be the smallest if she were about 35-50 years old. image image image 6.4.1.3 Observation with high prediction The patient with high expected health expenses value is a 66 years old women. She is married and have a bachelor degree. Her health expenses are equal to 36 295 (9.557 to the power of 3) and the models prediction is equal to 34 658 (3 to the power of 9.515). The patient suffers from many conditions including cancer, high blood pressure, angina, heart disease (had heart attack), high cholesterol, diabetes, joint pains. She has social and physical functioning limitations. She has a private insurance. Break Down explanations of model prediction for the patient. Break Down explanations indicates that the patient’s diseases have significant positive impact on the prediction. Shap explanations of model prediction for the patient. Ceteris Paribus explanations of model prediction for the patient. When investigated changes in the patients descriptions variables there are some additional interesting results. Despite patient positive diagnosis for many diseases, the health expenses would be lower if the patient age (Ceteris Paribus plot of the variable AGE31X) would be much more younger (20 years old or less). Patients walking limitations (Ceteris Paribus plot of the variable WLKLIM31) increases the prediction up to 1.73 (3 to the power of 0.5) times. Her low result in physical component summary (Ceteris Paribus plot for the variable PCS42) also increases the expenses cost (by up to 3 to the power of 1.5 times). Ceteris Paribus plot of the variable INSCOV15 shows the relation between type of insurance and the prediction. The patient has the private insurance (value 1) and if she had changed to public or opted for none she would have lower prediction. 6.4.1.4 Observation with low prediction The patient is a 21 years old man. He has never been married. He has only graduated from high school. He is poor. He has not have any insurance. He perceives his health as at excellent state. He smokes but he has not been diagnosed with any diseases. His health expenses are equal to 0.0. Model’s prediction is equal to 3 to the power of 0.004. Break Down explanations of model prediction for the patient. The Break Down explanations indicates his overall ratings of feeling may increase the prediction but after considering all the other factors the model prediction is equal almost to 0. Shap explanations of model prediction for the patient. Shap explanations shows that his not student status lowers the prediction by 3 to the power of 0.26 times. His good eyesight, no walking limitations and lack of heart diseases lowers the prediction 3 time to the power of around 0.2 times. Generally his lack of diseases lowers the predictions. Ceteris Paribus explanations of model prediction for the patient. If the patient had been diagnosed with diabetes his health expenses would be 3 to the power of 2.5 times higher (plot of the variable DIABDX). Positive diagnoses for high cholesterol would also increase by 3 times the health expenses prediction. What is more, he is not a student, what results in lower prediction by up to 3 to the power 1.3 times (plot of the variable FTSTU31X). His poverty status decreases the prediction by almost 3 times. 6.4.2 Dataset level In this part the focus is put on model-level explanations. Variables that are interesting, intuitive or simply important from the model perspective will be investigated. 6.4.2.1 Variable importance Figure presents permutational variable importance of ten most important features for XGB model. 6.4.2.2 Age variable Intuitively, the age of the patient should be a very good predictor. Even non-experts can tell that usually the older the person is, the more issues with health it has and thus - the more money will be spend on healthcare. Lets have a look at PDP and ALE profiles of this variable in our model: ALE and PDP profiles of age variable Both profiles follow similar trends and in general they show the expected behaviour. Some people are born with health issues already, so they generate high costs at the very beginning of their lives. Then, the ones who manage to overcome these initial issues are generally healthy. Usually teenagers are in a good shape - a lot of physical activity, careful parents, regular eating at schools etc. Once they become adults (16-18) their life becomes riskier (driving license, alcohol). Some of them also start to earn their own money. All of this sums up to sharp increase in medical expenditures of this group. Then there’s another plateau and another sharp increase at the age of about 50 years. This the age when a lot of disease are getting more probable thus people are taking clinical tests, diagnose and start to cure. This is also the age when menopause happens to women which usually worsens their health status. From now on, health status becomes worse and health expenditures are usually getting higher every year. From the model developer perspective - this relation seems to be a little raged. One would expect to be more smooth and monotonic. For example, there is no particular reason why medical expenditures should decrease at once we turn 60, but the plot shows otherwise. This suggests that it might be a good idea to discretize the age variable into number of groups. While there is very small probability that your health status will get worse next year, it is highly likely that it will get worse 10 years from now. It is worth investigation whether this transformation of the variable will improve the results. From the perspective of the medical provider and patient itself - we can clearly see that there are some points at which the expenditures sharply increase. It might be good idea for a patient to buy additional insurance once he or she approach that age. On the other hand, insurance provider should take that into account while preparing his offer. If the agreement time is couple of years and it includes that specific time when health sharply worsens, this should be included in price. 6.4.2.3 Evaluation of health status Evaluation of one’s health status seems to be an obvious indicator of what can we expect in terms of health expenditure. One should be cautious though - the methodology of this measurement is crucial here. Let’s have a look at profiles of variables PCS42 and RTHLTH31 - both are self-evaluation of ones health. PCS42 was created based on set of concrete questions about specific pain, limitations in activities etc. The higher the value, the better the health. On the other hand, RTHLTH31 just describes the overall health condition in one word, corresponding to the scale from 1 to 5 (“excellent”, “very good” “good” “fair” and “poor”). In this case, the higher the value, the worse the health. Both plots show the expected behaviour of the model. It is interesting to see though, that there is not much difference between “excellent” and “very good”. Similarly - the difference is very small between “fair” and “poor”. This is primarily caused by the subjective matter of these answers. First of all the understanding of the words their selves - something which is very good for one person, might be just good for other. Second thing is that we usually compare our current health to the recent past. If one feels tired every day, he might say that his health is fair. But if he feels a little tired every day but he just recovered from flu, he will say he feels very good, or excellent (in comparison to how he felt a week ago). Finally, people tend to get used to their diseases. It is proven, that people feel very bad at the beginning - when they are diagnosed, but when the time goes by they care less and less - they just get used to living with a disease. Nevertheless, it is important to say that insurance provideres should be very careful while pricing their services partially basing on surveys. It is crucial to ask specific questions that cannot be biased by subjective feelings of the respondent. 6.5 Summary and conclusions References "],
["story-lungs.html", "Chapter 7 Story lungs: eXplainable predictions for post operational risks 7.1 Introduction 7.2 Model 7.3 Explanations 7.4 Summary and conclusions", " Chapter 7 Story lungs: eXplainable predictions for post operational risks Authors: Maciej Bartczak (UW), Marika Partyka (PW) Mentors: Aleksandra Radziwiłł (McKinsey &amp; Company), Maciej Krasowski (McKinsey &amp; Company) 7.1 Introduction Science allows us to understand the world better. New technologies, data collection solves the problems not only of large companies but also of ordinary people. Especially if human life is at stake. They say that cancer is the killer of the 21st century. That’s why even small attempts to subdue this problem are important. In our work, we deal with lung cancer. We try to predict the chances of survival of a patient who has had a tumor removal surgery. We try different approaches. 7.2 Model The dataset consists of the following varaibles. Numerical Variable Unit date_birth date date_start_treatment date date_surgery date tumor_size_x cm tumor_size_y cm tumor_size_z cm years_smoking years age years time_to_surgery years (“today” - date_surgery) Categorical Variable Decription Values sex subject sex male/female histopatological_diagnosis type of cancer Rak płaskonabłonkowy, pleomorficzny, …* symptoms whether symptoms were observed yes/no lung_cancer_in_family whether family member had cancer yes/no stadium_uicc severity of tumor IA1, IA2, IA3, IB, IIA, IIB, IIIA, IIIB, IVA, IVB* alive whether subject is alive yes/no (target variable) About 1/3 af values of varaibles annotated with * was missing. Survivability was registered time_to_surgery years after the procedure, for the majority of the subjects not later than 1 year after the procedure, and at most after 12 years. We have tried out several models as well as different preprocessing strategies. However, all of the approaches yielded similar results differing no more than 0.01 of cross validation acuracy and 0.01 ROC AUC score. These are the models that were utilized: Logistic regression Logistic regression with hyperparameters cross validation Random Forest XGBoost Neural Net - 10 hidden neurons Neural Net - 30 hidden neurons Neural Net - 2 x 10 hidden neurons As we have indentified higle defendent features as well as observed that tumor sizes disturbed the explaination we have employed following preprocessing strategies: encode and normalize encode, remove highly dependent features and normalize encode, introduce tumor volume, discard tumor sizes, remove highly dependent features and normalize Finally we have settled on Neural Net with 10 hidden neurons with following receiver operating curve. 7.3 Explanations We are based on 3 methods of explaining models, mainly Ceteris Paribus as well as Shap and Variable Importance. Of course, it is our goal to understand on what basis our model has found a chance of survival of a given patient. For example, let’s take a patient with the following variable values: - date_start_treatment 2007-03-01 - sex M - histopatological_diagnosis rak niedrobnokomórkowy - years_smoking 35 - lung_cancer_in_family No - symptoms No - stadium_uicc IIIA - age 54 - time_to_surgery 0 - volume 125 Our most recent model indicated that the chances of survival after surgery for such a patient are \\(24\\%.\\) Here we have an explanation of this result by SHAP. According to our intuition, the lack of symptoms increases the chance of survival, quite advanced stage of UICC equal to IIIA reduces these chances. We may also notice that the prognosis is worse because our patient is a man. However, it will be best if we refer to the results of another method - Ceteris Paribus. Here we see 3 variables that show well how a parameter change works for or against the patient. This confirms that women have a better prognosis of survival after surgery, the symptoms of the disease do not herald the best, and also the younger we are, the better we are able to cope with convalescence. Here too, the model shows that if our patient’s cancer was classified as much lighter, his chances would increase. Let’s take a look at another method for the whole data set - Variable Importance. The most important one seems to be the variable that says when the patient started treatment. The second most important is the UICC stage. Interestingly, the period of time the patient smoked cigarettes does not affect the outcome too much. But using model explanations not only helps to explain the result of the prediction, it can also give a hint how to improve our model. When we built the model on all variables, the explanations allowed us to find correlations. Let’s compare the CeterisParibus results for the full model and the deleted dependent variables. In the picture above you can see that the influence of two correlated variables was distributed between them, but after leaving only one of these variables, the influence accumulated on it (picture below). At this stage we can already conclude that the techniques of model explanations are not only useful at the end of our journey. They can give us tips on how to transform data or which variables should be deleted. 7.4 Summary and conclusions XAI methods legitimised employed approach of pruning the dataset. XAI methods yielded explainations consistent with biological intuintion, what builds up trust in the model. As variety of modelling and preprocessing approaches resulted in similar predicitive performance we conclude there is not much more to squeeze out of the dataset. "],
["heloc-credits.html", "Chapter 8 Heloc credits 8.1 Introduction 8.2 Dataset 8.3 Model 8.4 Explanations 8.5 Application for clients 8.6 Summary and conclusion", " Chapter 8 Heloc credits Authors: Tomasz Kurzelewski (University of Warsaw), Tomasz Radzikowski (Warsaw University of Technology) Mentors: Marta Gajewska (McKinsey &amp; Company), Amadeusz Andrzejewski (?) (McKinsey &amp; Company) 8.1 Introduction A home equity line of credit, or HELOC, is a loan in which the lender agrees to lend a maximum amount within an agreed period (called a term), where the collateral is the borrower’s equity in his/her house (akin to a second mortgage). Because a home often is a consumer’s most valuable asset, many homeowners use home equity credit lines only for major items, such as education, home improvements, or medical bills, and choose not to use them for day-to-day expenses. Since amount of such credit is not small, banks carefully review financial situation of applicants. Utmost care is taken so the whole process is transparent and decision is easily explainable to the client. Because of that any automated process also has to be explainable, and in this XAI methods may be helpful. 8.2 Dataset Our dataset - Home Equity Line of Credit (HELOC) - originally cames from Explainable Machine Learning Challange organized by FICO company. The data contains anonymized credit applications of HELOC credit lines, which are a type of loan, collateralized by a customer’s property. There are 23 predictors in the dataset, which describe following features: * ExternalRiskEstimate - consolidated indicator of risk markers (equivalent of polish BIK’s rate) * MSinceOldestTradeOpen - number of months that have elapsed since first trade * MSinceMostRecentTradeOpen - number of months that have elapsed since last opened trade * AverageMInFile - average months in file * NumSatisfactoryTrades - number of satisfactory trades * NumTrades60Ever2DerogPubRec - number of trades which are more than 60 past due * NumTrades90Ever2DerogPubRec - number of trades which are more than 90 past due * PercentTradesNeverDelq - percent of trades, that were not delinquent * MSinceMostRecentDelq - number of months that have elapsed since last delinquent trade * MaxDelq2PublicRecLast12M - the longest delinquency period in last 12 months * MaxDelqEver - the longest delinquency period * NumTotalTrades - total number of trades * NumTradesOpeninLast12M - number of trades opened in last 12 months * PercentInstallTrades - percent of installments trades * MSinceMostRecentInqexcl7days - months since last inquiry (excluding last 7 days) * NumInqLast6M - number of inquiries in last 6 months * NumInqLast6Mexcl7days - number of inquiries in last 6 months (excluding last 7 days) * NetFractionRevolvingBurden - revolving balance divided by credit limit * NetFractionInstallBurden - installment balance divided by original loan amount * NumRevolvingTradesWBalance - number of revolving trades with balance * NumInstallTradesWBalance - number of installment trades with balance * NumBank2NatlTradesWHighUtilization - number of trades with high utilization ratio (credit utilization ratio - the amount of a credit card balance compared to the credit limit) * PercentTradesWBalance - percent of trades with balance Features containing data about delinquency are coded to numeric scale and missing values are labeled with negative integer number. The majority of features are monotonically decreasing or increasing. Dataset has ..xxx.. observations, ..xxx.. of its belongs to class ‘Good’, what means that clients repaid their HELOC account within 2 years, and ..xxx.. to class ‘Bad’. 8.3 Model Since credit decision takes into account many variables related to the customer’s financial situation, and many of them were not included into dataset, accuracy of built models was not as good as we would want. Best results available in papers are around 0.84 AUC, while our model based on XGBoost scored around 0.80 AUC. We experimented with a different models, such based on SVM, Random Forest and XGBoost. The last two gave best results, both around 0.80 AUC, but we chose XGBoost since available implementation allowed us to reinforce monoticity bounds arising from business interpretation of variables. It should provide us with model better describing decision process. We also experimented with variable ‘ExternalRiskEstimate’. This variable representing external credit scoring must be based on other variables from the dataset, and because of that many trained models were relying almost exclusively on this single variable, marginalising importance of other ones, and oversimplifying explanation. Such explanation wouldn’t be in any way meaningful to potential applicant. What’s more, ExternalRiskEstimate can be explained with other variables with mean absolute error 2.5. 8.4 Explanations SHAP values: [alt text][images/08_shap1.png] 8.4.1 Explanation for management Since management is more focused on overall financial situation, they will get more information from data based on whole dataset than individual instances. One of the most important information is which variable are most important in our model. To uncover it we can use Permutation Feature Importance. In our case it were NetFractionRevolvingBurden, AverageMInFile and PercentTradesNeverDelq. 8.5 Application for clients While working on this project we tried to use SHAP values to change the particular feature in the observation, what should increase or decrease probability of a positive credit decision. Results was surprisingly bad: in some cases changing the most important feauture in a reasonable range did not affect the output. In others, even a small change disrupted the whole SHAP model. [alt text][images/08_ERE_pc.png] 8.6 Summary and conclusion Our results show that explainable artificial intelligence could be helpful for banking industry and could provide a valuable explanaition for clients, what is necessary in many countries, including Poland. Unfrotunatelly there are some drawbacks of those techniques, what was shown in previous sections. Although SHAP values present current client’s situation, it is not possible to modify values of a certain feature causing a monotonical increase of probability of a positive credit decision. "],
["story-uplift-marketing1.html", "Chapter 9 Story: Uplift modeling - eXplainable predictions for optimized marketing campaigns 9.1 Introduction 9.2 Dataset 9.3 Model exploration and metrics 9.4 Explanations 9.5 Conclusions 9.6 Summary 9.7 Future works", " Chapter 9 Story: Uplift modeling - eXplainable predictions for optimized marketing campaigns Authors: Jan Ludziejewski (Warsaw University), Paulina Tomaszewska (Warsaw University of Technology), Andżelika Zalewska (Warsaw University of Technology) Mentors: Łukasz Frydrych (McKinsey), Łukasz Pająk (McKinsey) Key points: uplift models thanks to its additivity give wide range of possiblities while using XAI SHAP values can be used to explain model both in local and global aspects - they can be generalized in order to estimate Variable Importance and Partial Dependence Plots in the case of analysed dataset, marketing campaign should be sent two months after last purchase to be the most effective XAI analysis can help in creating personalized marketing campaigns 9.1 Introduction Running a business is a challenge. It involves making a lot of decisions to maximize profits and cut down costs - finding the tradeoff is not a straightforward task. Here come Machine Learning and uplift models that can help in optimizing marketing costs. It is widely believed that it is good idea to send marketing offer to all company’s customers. From one point of view we think the probability that the customer will buy our product is higher - in fact it is not always the case (the matter will be described in details later). On the other hand, making a large-scale campaign is costly. Therefore it is important to consider what is the Return on Investment (ROI) in decision-making. Is it true that by sending the marketing offer we only extend the chance for the customer to buy our product and therefore extend our profit? The issue was already investigated (Verbeke and Bravo 2017) and it was pointed out that customers of any company can be divided into 4 groups (Figure: 9.1). FIGURE 9.1: Customer types taking into consideration their response to treatment (???) The matrix (Figure: 9.1) was created based on customer decision to buy a product depending on the fact that they were addressed by a marketing campaign or not. The action used for triggering in customers the particular behavior is called treatment. In the 4 groups we distinguish: “persuadables”: the customers that without being exposed to marketing campaign would not buy a product “sure things”: the customers that irrespective of the fact that they experienced treatment or not are going to buy a product “lost causes”: the customers that irrespective of the fact that they experienced treatment or not are NOT going to buy a product “sleeping dogs”: the customers that without being exposed to marketing campaign would buy a product but in case they receive a marketing offer they resign It can be then observed that in case of “lost causes” and “sure things” sending a marketing offer makes no impact therefore it doesn’t make sense to spend money on targeting these customers. As the company we should however pay more attention to the groups “persuadables” and “sleeping dogs”. In the case of the first group, bearing the costs of the marketing campaign will bring benefits. In the case of the latter, we not only spend money on targeting them but as a result, we will also discourage them from buying the product therefore as a company we loose twice. The case of “sleeping dogs” can seem irrealistic, therefore we present an example. Let’s imagine there is a customer that subscribed to our paid newsletter. He forgot that he pays each month fixed fee. He would continue paying unless a company sends him a discount offer. At this moment the customer realizes that he doesn’t need the product and unsubscribes. By understanding the structure of the customers, company can target its offer more effectively. 9.1.1 Approaches towards uplift modeling In (Akshay Kumar 2018) it was pointed out that the problem of deciding whether it is profitable to send an offer to a particular customer can be tackled from two different perspectives: predictive response modeling (it is common classification task where model assigns a probability to each of the classes) uplift modeling (where the “incremental” probability of purchase is modeled) The latter is tailored to this particular task and is more challenging. Uplift modeling is a technique that helps to determine probability gain that the customer by getting the marketing materials will buy a product. The field is relatively new. The two most common approaches are (Lee 2018): Two Model In this method two classifiers are build. The one is trained on observations that received treatment (called model_T1) and the second is trained on observations that didn’t receive treatment (called model_T0). Later the uplift for particular observations is calculated. If the observation experienced treatment then it is an input to the model_T1 and the probability that the customer will buy a product is predicted. Next it is investigated what would happen if the customer didn’t receive treatment. In that case the treatment indicator in observation’s feature is changed to “zero”. Such a modified record is an input to model_T0 that predicts the probability that such a customer will buy a product. The uplift is calculated as the difference between the output of the model_T1 and model_T0. The higher the difference, the more profitable is addressing marketing campaign to a particular customer. Analogically, uplift is computed for the people that didn’t experienced treatment. One Model This approach is similar conceptually to the Two Model approach with such a difference that instead of building two classifiers only one is used. Therefore every observation is an input to the model that generates prediction. Later the indicator in the treatment column is changed into the negation and such a vector is used as input to the model that once again output probability that the customer buys a product. The uplift is the difference between the two predicted probabilities. FIGURE 9.2: Two Model vs One Model approach (own elaboration), where P1=P(purchase|T=1) and P0=P(purchase|T=0) As uplift modeling is an emerging field there isn’t a clear evidence what method is better to use. 9.2 Dataset There is a scarcity of well-documented datasets dedicated to uplift modeling. Therefore the authors of (Rzepakowski and Jaroszewicz 2012) proposed to artificially modify available datasets to extract information about treatment. As the purpose of this story is to investigate XAI techniques in the domain of uplift modeling we decided to use real-life dataset. We chose Kevin Hillstrom’s dataset from E-Mail Analytics And Data Mining Challenge (Hillstrom 2008). The dataset consists of 64000 records reflecting customers that last purchased within 12 months. As a treatment an e-mail campaign was addressed: 1/3 of customers were randomly chosen to receive an e-mail campaign featuring Men’s merchandise 1/3 were randomly chosen to receive an e-mail campaign featuring Women’s merchandise 1/3 were randomly chosen to not receive an e-mail campaign (“control group”) As an expected behavior the following actions were determined: visit the company’s website within 2 weeks after sending to the customers a marketing campaign purchase a product from the website within 2 weeks after sending to the customers a marketing campaign In the challenge the task was to determine whether the men’s or women’s e-mail campaign was successful. In order to simplify the problem we reformulated the task - we have focused on answering the question whether any e-mail campaign persuaded customers to buy a product. The features about customers in the dataset are specified in Figure 9.3: FIGURE 9.3: Customer features in the dataset (own elaboration) In the dataset there is also information about customer activity in the two weeks following delivery of the e-mail campaign (these can be interpreted as labels): Visit: 1/0 indicator, 1 = Customer visited website in the following two weeks Conversion: 1/0 indicator, 1 = Customer purchased merchandise in the following two weeks Spent: Actual dollars spent in the following two weeks 9.2.1 Explanatory Data Analysis First we decided to investigate variable that have more than 3 unique values. At the same time these variables (recency and history) intuitively seem to be the most important while predicting whether someone will buy a product or not. FIGURE 9.4: Histograms of recency (left) and history (right) It can be seen that history variable has heavy-tailed distribution therefore it may be reasonable to use Box-Cox transformation. However, we decided to keep the variable without any preprocessing for easier interpretation. FIGURE 9.5: Count plots In case of mens, womens and newbie variables the proportion of 0’s to 1’s is almost equal. There is much fewer record of people living in countryside than in urban or suburban area. Most of the company customers buy via phone or web. It is rare that someone uses mulitchannel option. In the dataset most of the customers received treatment in the form of marketing E-mail. 9.2.2 Feature engineering Regarding labels the dataset is largely imbalanced - there are only about 15% of positive cases in column Visit and 9% in column Conversion. In such a situation we decided to use column Visit as a target for the classifier. As the number of columns is small we decided to use one-hot encoding for transforming categorical variables instead of target encoding. 9.3 Model exploration and metrics There are not many packages dedicated to uplift modeling in python. We investigated the two: pylift (“Pylift Package - Documentation,” n.d.) and pyuplift (???). The latter enables usage of 4 types of models - one of those is the Two Model approach. In the pylift package there is the TransformedOutcome class that generates predictions. However, the model itself is not well described and uses XGBRegressor underneath that is not intuitive. Fortunately the package offer also the class UpliftEval that allows uplift metrics visualization. In the scene, we decided to create own classifier (as in the One Model approach) and use UpliftEval class from the pylift package for metric evaluation. As the classifier we used fine-tuned XGBoost with the score function as the area under the cumulative gain chart (described below). In the figure 9.6 we show the cumulative gain chart for train and test sets. FIGURE 9.6: Cumulative gain chart: (left) train set, (right) test set The Qini curve is not suggested for performance evaluation of uplift models as it is vulnerable to overfitting to the treatment label. Therefore the Cumulative gain chart is used. It is the least biased estimate of the uplift. In the pylift package it is implemented based on the formula: \\(Cumulative\\ gain(\\phi) = (\\frac {n_{t,1}(\\phi)}{n_{t,1}}-\\frac {n_{c,1}(\\phi)}{n_{c,1}})(\\frac{n_t(\\phi)+n_c(\\phi)}{N_t+N_c})\\) where \\(n_{t,1} (\\phi)\\) is the number of observations in the treatment group at cutoff level \\(\\phi\\) with label 1 \\(n_{c,1} (\\phi)\\) is the number of observations in the control group at cutoff level \\(\\phi\\) with label 1 \\(n_{t,1}\\) is the total number of observations in the treatment group with label 1 (analogically \\(n_{c,1}\\)) \\(N_t\\) is the total number of observations in treatment group (analogically \\(N_c\\)) The theoretical plot is created according to the following scheme: First the customers are sorted in descending order based on predicted uplift. Later some fraction of data is taken for the analysis (e.g. 10% of the people with the highest score). This cutoff is represented as \\(\\phi\\) in the formula. Next the uplift gain is verified for the subset. At the beginning of the curve the gain is the biggest as it refers to the “persuadables” group. Later the curve stabilizes as it depicts the groups: “lost causes” and “sure things”. At the end the curve decreases as there are “sleeping dogs” with negative uplift. 9.3.1 Model It can be seen that our model is better than random choice but much worse than the practical/theoretical maximum possible. It is also worse than the case without “sleeping dogs”. However we have to remember what this uplift score indicates. Maximum uplift would mean, that we can always persuade anyone to buy, which is obviously an unrealistic scenario, since our treatment is only a single advertisement. Comparing to uplift modeling in different domains, i. e. medical usages, our treatment has generally smaller impact on individual, therefore dataset itself is more noisy and cumulative gains are smaller. It is also worth noting, that due to small number of features, there are multiple cases in the dataset where two observations with same features have different answers. This kind of noise in data also tremendously impacts the score and requires caution when training models. Also, since uplift itself is an interaction, our model do have to take them into consideration. Taking previous observations into consideration, model was found using so called local search procedure, what means that we choose some meta-parameters of the model and iteratively, for every meta-parameter approximate derivative by sampling the local neighbourhood of current value and follow the ascending gradient. Local search stops naturally, when in previus iteration, we did not change any parameter, hence we hit one of the local minima. To be clear, if meta-paremeter is discrete, by approximating local neighborhood we mean just checking close values. For our score function, we’ve chosen cross validation on cumulative gains. This kind of procedure should seek for highly robust models. Therefore, it is worth noticing that our model didn’t experience any overfitting as its quality on the train and test sets is similar. The resulting major parameters were: maximum depth of 5, high learning rate of 0.7 and only 12 estimators. 9.3.2 Comparing metrics We tried to employ the same local search procedure (as described in Model section) using accuracy as score function. However, it failed to converge with any decent quality, because this metric is much less informative in case of largely imbalanced dataset. Since only a small number of customers actually purchased, it’s hard to correctly predict a positive case using non-overfitted model. Therefore within the local search with accuracy function, starting local neighborhood was always flat. This might be because in the dataset there is more noise than positive cases. But fortunately, only important factor, from our prespective is probability of the purchase, since uplift is increase of purchase probability after treatment, and it directly transfers into money gain. To visualize it in a straightforward manner, we present a table comparing our current robust XGBoost model with overfitted one (deep trees, 100 estimators). TABLE 9.1: Metrics comparison Model Train accuracy Valid accuracy Train cummulative gain Valid cummulative gain Overfitted XGBoost 0.8755 0.8473 0.7190 0.0204 Robust XGBoost 0.8532 0.8532 0.0398 0.0425 As we can see (Table 9.1) for the overfitted model, while the overfit gap in accuracy scores is only around 2%, cummulative gain drop by 97%. 9.4 Explanations The Cumulative gain chart (Figure 9.6) shows that the proposed model brings additional value. Here comes the question of whether the model is reliable? Does it make the decision based on the features that are important from an expert perspective? Such judgment can be done based on the results of XAI tools. We decided to investigate model interpretability from instance-level and dataset-level perspective. 9.4.1 Instance-level In order to explain model output for particular customer we employed SHAP (SHapley Additive exPlanations) values (Lundberg and Lee 2017). Before we move to the investigation of SHAP values let’s get to know customers that got the highest and the lowest uplift prediction. In this section we will analyze the reliability of predictions for these particular instances. In the table 9.2 there is all the information provided to the system about the customers. TABLE 9.2: Customer with the highest and the lowest uplift - features Column.name customer_with_biggest_uplift customer_with_lowest_uplift recency 2.0 5.0 history 228.93 243.95 mens 1.0 0.0 womens 1.0 1.0 zip_code_Surburban 1.0 0.0 zip_code_Rural 0.0 1.0 zip_code_Urban 0.0 0.0 newbie 0.0 1.0 channel_Phone 0.0 1.0 channel_Web 1.0 0.0 channel_Multichannel 0.0 0.0 segment 0.0 1.0 As can be seen (Table 9.2) the customers spend almost the same amount of money during the last 12 months on our company’s products. The person with the highest uplift did last shopping 2 months ago whereas the person with the lowest uplift did it 5 months ago. In the dataset some people purchased the product for the last time even 12 months ago so the person is not the most severe case in that sense. Apart from many other differences among the two customers the key is that the person with the highest uplift received treatment whereas the second customer didn’t. Below we present SHAP values for the customer desribed in Table 9.2. The values were computed directly on uplift model 9.7. FIGURE 9.7: SHAP values: (left) customer with the lowest uplift, (right) customer with the highest uplift In can be seen that in both cases big contribution to the final result has information about customer history (about 235 USD) and the fact that the customer bought products from Womens collection. What is interesting is the fact that the customers have almost the same values of these two attributes but opposite sign of its contribution. We can benefit from additive feature attribution property of SHAP values to model the uplift: \\(uplift=P(purchase|\\ T=1) - P(purchase|\\ T=0))\\) \\(SHAP(uplift)= SHAP(P(purchase|\\ T=1)) - SHAP(P(purchase|\\ T=0))\\) This property gives us a great opportunity to evaluate these two vectors of SHAP values independently. For example if we use any tree-based model, we can make use of tree-based kernel for SHAP value estimation (faster and better convergent) instead of modeling it directly as a black-box (uplift) model. In a table 9.3 there is a comparison of SHAP values obtained using two methods for the customer with the lowest uplift. TABLE 9.3: SHAP values obtained using two methods Column_name Uplift_approach Diff_approach recency -0.00593 -0.00535 history -0.27282 -0.27270 mens -0.00961 -0.00928 womens -0.05628 -0.05692 zip_code_Surburban 0.00076 -0.00055 zip_code_Rural -0.03313 -0.03257 zip_code_Urban -0.00148 -0.00179 newbie 0.00365 0.00351 channel_Phone 0.00024 -0.00036 channel_Web 0.00188 0.00200 channel_Multichannel -0.00067 0.00029 segment 0.00010 0.00043 Experimental results proved that these two ways of calculating SHAP values are providing close estimations with precision to numerical errors. There are few features that depending on method have small positive or negative value. This is caused by the fact that for the estimation of SHAP values directly using uplift model the KernelExplainer was used. It has data parameter however in case of big dataset it is recommended in the documentation to take subset of records. In our case we took random subset therefore this could impact the results. Nevertheless, we proved that in case of our example the two methods lead to similar values. The specificity of uplift models in terms of the possibility to analyse them through additivity of SHAP values gives room for another valuable inspection. Below we present how SHAP values differ depending on the fact that the customer was or wasn’t addressed by treatment. On x axis there are SHAP values in case T=0 and on y axis in case T=1. In each chart there is SHAP value referring to one variable. In situation when the SHAP values are the same irrespective of the presence or absence of treatment they would lie on identity line. Moreover, there is color used as third dimension indicating the group that the particular customer belongs to. We decided to merge two groups (“sure things” and “lost causes”) as they have uplift almost equal to zero, therefore now we can distinguish 3 groups: “sleeping dogs”, “persuadables” and “sure things and lost causes”. The division was based on the predicted uplift. “Sleeping dogs” have considerable negative uplift, “sure things and lost causes” have uplift in \\([-0.01,0.01]\\) and “persuadables” have uplift greater than 0.01. The group “sure things and lost causes” should has zero uplift, but due to numerical issues we decided to set \\(\\epsilon\\) equal to 0.01. As almost all customers were categorized to “persuadables”, we decided to show on the plot only 1000 records from this group to maintain chart readability. FIGURE 9.8: SHAP values on variable recency in case T=0 and T=1 It can be seen that “persuadables” are slightly above and below identity line. FIGURE 9.9: SHAP values on variable history in case T=0 and T=1 In Figure 9.9 the three customer groups are distinctive. It would be interesting whether the result of clustering metohds would be similar. We also investigated binary variables. Most of them looked similar as Figure 9.9 but there was one exception - variable womens. FIGURE 9.10: SHAP values on variable womens in case T=0 and T=1 The customer groups on Figure 9.10 are overlapping. They constitue very homogeneous groups. Note: In the case of our model there is no need to apply LIME as its main advantages - sparsity - is not important when there are only few columns. 9.4.2 Dataset- (subset-) level Unfortunately, it’s impossible to calculate directly Permutation Feature Importance, because of the previously mentioned problem with lack of full information. We don’t know if the client purchase product after treatment or will he buy without treatment as well. Because of having in disposal only historical data (not an oracle), we have only one of these two pieces of information. However, we can make use of the previously computed SHAP values of uplift to calculate the same value of permutational feature importance as an average of local SHAP importance (defined in a permutational way itself, however, calculated more smartly (Lundberg and Lee 2017)). We decided to evaluate feature importance not from the well-known dataset-level but subset-level. As subsets we perceive the 3 customer groups: “sleeping dogs”, “sure things and lost causes” and “persuadables”. Below we present the Variable Importance plots. The correlations between shap variables of particular variable and variable itself were highlighted in colors. The red color means a positive correlation whereas blue means negative correlation. FIGURE 9.11: Variable Importance - “sleeping dogs” FIGURE 9.12: Variable Importance - “sure things and lost causes” FIGURE 9.13: Variable Importance - “persuadables” Conclusions: Regardless of the customer groups always history and womens are among three most important features. For observations with considerable negative uplift (“sleeping dogs”) both history and womens have negative correlation with their SHAP values. In case of “sure things and lost causes” womens has positive correlation whereas history has negative. The same variables among “persuadables” (considerable positive uplift) have positive correlation with SHAP values. Correlation changes gradually with uplift value. It is worth noticing that regardless of the customer group importance of segment variable is low. This is suprising because the presence of treatment is substantial element of uplift modeling. What is interesting is the fact that regarding zip code only the information whether someone is from rural area is important. Note that this category of dwelling place was the least popular among customers. Information about purchase channel in general has relatively small predictive power. 9.4.2.1 Dependence plots Another tool to investigate model are the dependence plots. There are two options. The most common method is the Partial Dependence Plot (PDP)/ Accumulated Local Effects (ALE) and the another one is the SHAP dependence plot. The Partial Dependence Plot shows the marginal effect that one or two features have on the predicted outcome of a machine learning model (???). It tells whether the relationship between the target and a feature is linear, monotonic or more complex. In the SHAP dependence plot we can show how a feature value (x axis) impacted the prediction (y axis) of every sample (each dot) in a dataset (???). This provides richer information than the traditional Partial Dependence Plot, because we have at least two additional information: density and variance of observations. The Partial Dependence Plots reflect the expected output of the model if only one feature value is changed and rest stays the same. In contrast, the SHAP value for a feature represents how much that feature impacted the prediction for single sample, accounting for interaction effects. So while in general you would expect to see similar shapes in a SHAP dependence plot and a Partial Dependence Plot they will be different if your model has multi-variable interaction effects (like AND or OR). A PDP has no vertical dispersion and so no indication of how much interaction effects are driving the models predictions (???). We generated the Partial Dependence Plot for all features and the SHAP dependence plot based on 1000 observations only for history and recency features due to large processing time. FIGURE 9.14: History (left) Partial Dependence Plot / Accumulated Local Effects plot, (right) The SHAP Dependence Plot based on 1000 observations For our model the SHAP Dependence Plot reflects the shape of the Partial Dependence Plot. Contribution of history to the final uplift prediction differs among people with the same value of history. It can be seen that there is a considerable peak on the chart for history value about 230 USD. However people that spent such amount of money have various SHAP values - some positive, some negative. This observation is not contradictory to PDP as in case of PDP we compute the average. Note that on SHAP dependence plot we displayed a sample of size 1000. FIGURE 9.15: Recency (left) Partial Dependence Plot / Accumulated Local Effects plot, (right) The SHAP Dependence Plot based on 1k observations Due to the fact that recency can have only one of 12 values, only 12 “clusters” can be seen on the SHAP Dependence Plot. Dispersion within the “clusters” shows how much the observations in our dataset differ. FIGURE 9.16: Gender It is suprising that the disproportion between the results shown in Figure 9.16 is so significant. In the PDP of mens feature the lines are almost flat meaning that regardless of the fact whether someone bought or not a product from mens collection the uplift prediction stays the same. FIGURE 9.17: Newbie According to the Figure 9.17 if a person is a newbie it is harder to encourage him/her to buy a product through marketing campaign. FIGURE 9.18: Dwelling place It can be seen that PDP of zip_code_Suburban and zip_code_Urban look very similar. They both have decreasing trend. Whereas PDP of zip_code_Rural has increasing trend. In this case it can be seen that ALE nd PDP crosses as they aren’t parallel, it means there is slight interaction in model. FIGURE 9.19: Channel The biggest gain in terms of uplift can be seen in case when the person uses Web channel. The PDP of Phone_channel is flat. 9.5 Conclusions Since Partial Dependence Plots are generally parallel to Accumulated Local Effects, we can safely assume that our model does not have (major) interactions. However this does not mean, that we can use some classifier without interactions, because here we model directly the uplift, which is difference between preditions and is itself an interaction. 9.5.1 Sweet-spot The most important observation here, should be that while at first glance we can only manipulate the treatment variable, dependence plots also give us opportunity to choose best time to contact the customer. Intuitively, recency function should be concave, aiming to find some “sweet-spot” between time when customer “just went out from the shop” and “forgot about this”. The Figure 9.15 is indeed concave but only for recency values between 1 and 4. For larger recency there is sinusoidal noise observed. This fluctuations can be interpreted as small overfitting. The key message is that the sweet-spot appears to be two months after last purchase. 9.5.2 Bigger influence Based on Figure 9.16 bigger influence can be made on customers who bought womens products than the ones who bought products from mens collections. Initially we removed from the dataset information about treatment type (Woman’s/Man’s e-mail). But based on Variable Importance analysis (Figures: 9.11, 9.12, 9.13) we can reason about the type of e-mail that maximizes uplift for particular person. Considering how important is womens variable, we propose a following policy: regardless of the fact whether someone buys from mens or womens collections we should send e-mails dedicated to women. 9.5.3 Influence from other factors Other PDPs can be used for better understanding of the customer persuadability. Since the only variable considerably reducing estimated uplift is newbie we can safely conclude, that marketing campaigns have better impact on regular customers, which is quite an intuitive conclusion. Analysing other factors, one-hot encoded area of living (zip_code_Rural, zip_code_Suburban, zip_code_Urban) do not have influence bigger than statistical error maybe except zip_code_Rural. Customers living on countryside are more likely to be persuaded. Suprisingly it is the only factor that may have some interactions. Referring to purchase channel, it is best to target customers who bought only by web in past months. It may be connected to the fact that our treatment is conducted via e-mail. We suspect that in some cases the following situation can happen: someone buys via phone as he doesn’t use the internet often. In such cases e-mail campaigns will not be effective. 9.6 Summary Using XAI for uplift modeling helps to understand its complex models better. The analysis goes beyond just assessing whether the model is reliable. 9.6.1 Individual perspective In case of our task individual perpective doesn’t seem to be vital. The situation when a customer writes an e-mail to the company asking why he didn’t receive an e-mail with marketing campaign is highly unlikely. Even if he does, he wouldn’t change his feature like dwelling area only in order to get the e-mail. The things that the customer can rather easily change are his value of recency or history variable. 9.6.2 Data scientist perspective In the data scientist perspective the most important thing to check is whether the model is overfitted. The tool that can help in verifying model sensitivity is the Partial Dependence Plot. In case of our model it can be seen that the model is slightly overfitted as there is a peak on PDP of history. 9.6.3 Executive perspective XAI techniques can help the executives to understand better the company customers behavior without paying for some extra surveys to investigate their attitude towards the company. Key findings: The campaign e-mail should be send two months after last purchase in order to be more effective. The most important variables in the model seem to be resonable, e.g. history and recency (Figures: 9.11, 9.12, 9.13). The only suprising thing is high importance of zip_code_Rural feature. In the case of bigger number of treatment variants it would be possible to create personalized marketing campaign. A vital part of our work was adjusting XAI techniques for the particularities of uplift modeling. We found out that thanks to its additivity SHAP values are well suited for uplift modeling - we showed two methods of using it. We identified limitations of well-known Permutation Feature Importance in terms of explaining uplift modeling. It is caused by the fact that unlike in other supervised models here we do not have exactly labels. Therefore we used the generalization of SHAP values that converge to Permutation Feature Importance. Also we analysed the SHAP Dependence Plots as an alternative to PDP. We employed the analysis for the three groups of customers based on the corresponding uplift. 9.7 Future works During initial feature engineering, we simplified our problem by merging womens treatment and mens treatment into one. By analysing PDP we were able to propose a policy for choosing the optimal treatment type. However, it is not the only possible approach. We can try going beyond standard uplift modeling and model directly uplift with 2 possible outcomes i.e. create purchase prediction, and then check if sending womens or mens email is more profitable, resulting in following equation: \\(uplift=max{(P(purchase\\ |\\ w\\_T=1) - P(purchase\\ |\\ w\\_T=0,m\\_T=0), \\\\ P(purchase\\ |\\ m\\_T=1) - P(purchase\\ |\\ w\\_T=0,m\\_T=0))}\\) where: w_T is treatment dedicated to women m_T is treatment dedicated to men However, this leaves us with several open-ended questions i.e.: can we now implicitly calculate SHAP values, using previously presented effecient technique (based on additivity)? Surely the max function breaks the additivity of uplift function, but maybe it is possible using some other method? References "],
["hotel-booking-cancellation-story-explainable-predictions-for-booking-cancellation.html", "Chapter 10 Hotel booking cancellation story: eXplainable predictions for booking cancellation 10.1 Introduction 10.2 Problem specification 10.3 Target leak detection 10.4 Bias correction 10.5 Offering different conditions 10.6 Conclusions", " Chapter 10 Hotel booking cancellation story: eXplainable predictions for booking cancellation Authors: Miłosz Michta (University of Warsaw), Kazimierz Wojciechowski (Warsaw University of Technology) Mentors: Maciej Andrzejak (McKinsey &amp; Company), Alicja Jośko (McKinsey &amp; Company) Apr–Jun 2020 10.1 Introduction Imagine you are the owner of a portuguese hotel. You notice that some of the reservations are cancelled on a regular basis. To maximize revenue you plan to practice room overbooking strategy. How exactly would you approach this strategy? How many rooms to oversell? Which customers are most likely to cancel their reservation? What is the probability of cancellation? And most importantly: what is the reason behind the prediction? We will make an attempt to answer this question using explainable machine learning. From the standpoint of a machine learning engineer one might want to find a model that not only is high-performing, but is also based on intuitive insights about the data. A persuasive manager of the facility might want to attempt to offer the booking on slightly different conditions to customers that are most prone to cancel their reservation e.g. by offering a non-refundable booking for customers from Portugal and Germany. It will be of crucial importance to determine which features to tweak to make the booking as secure as possible while trying not to make the customer feel too uncomfortable by insisting on too many changes. To solve this interesting problem we propose instance-specific explainable machine learning techniques. 10.2 Problem specification In this chapter, we will use data from Hotel booking demand (Mostipak 2020) The data contains several information about when booking was made, what is the date of an arrival, how long visitors will stay, where are they come from, how many of them will come, etc. In terms of hotel, we know what type of the hotel is, what is the ADR, the deposit type, agent and company that made the booking or are responsible for paying booking, and much more. Our machine learning task will be classification whether client will cancell their bookings or not. From business perspective, model like this might not be explicite useful, since predicted category should not affect decision of cancellation from the side of company, agent or hotel itself. On the other hand, use of explainable machine learning might give us the most important factors of cancellation process and use them to reduce rate of cancelled reservations. To obtain most reliable results, we have used three different models: LightGBM (Guolin Ke 2017), Naive Bayes and Logistic regression. Each of them has distinct learning procedure and structure, thus our conclusions should not be biased by the choosed model. 10.3 Target leak detection At the beginning, we test how baseline models work without any feature engineering. Suprisingly, all models get 100% accuracy score both on training and validation score. This could mean three things: We are awesome! We are lucky! We have a target leakage in the dataset! Firstly, let’s see partial dependence profiles for each feature. If there is some leakage, then average prediction profile should distinguish itself amoung the others. At first glance, the reservation status feature has strange plot. It looks like we ommit the feature represents almost the same as target. Partial dependence profiles for reservation_status feature using three different models: LightGBM, Naive Bayes and Logistic regression. We had to remove this feature from our dataset even if we loose high performance score, because of laking interpretation power. 10.4 Bias correction Since we have already remove leakage feature, lets figure out which variables are the most important. The most important features for LightGBM, Naive Bayes and Logistic regression in according to Permutation Importance. It looks like, the country of origin is the most important and the most impactful for 2 out of 3 models. The others variables looks reasonable, but logically country should not be the most important feature for predicting booking cancellation. Lets figure out what what is the cause if results like that. Dependency between number of observations (angle) and average prediction (radius) for each country. The Portugal seems to be reason of the bias in the dataset. Around 40% of the data comes from Portugal and their average prediction equals 56%, where the average is around 38%. In this case, we split model on 2 parts: One model for portugals One model for other countries. Accuracy score uplift after splitting data for portugals and other countires. After splitting the dataset in each fold and training two distinct models, we get significant uplift in model accuracy for each model and fold. 10.5 Offering different conditions Here we will attempt to use a instance-specific explainable machine learning techniques in order to provide a similar offer that will reduce the probability of cancellation. We will determine which methods, if any, might be applicable for real-time negotiation. 10.6 Conclusions As you probably noticed, explainable machine learning gives a lot opportunities to validate predictive models, find most insightful facts about data and set new directions to improve our results. Explainaible machine learning methods can be compared to the pointing fingers on every model weakness and to the compass guiding where we should go to improve final outcome. Thank to XAI accomapnied by strong analysis and visualtion, we was able to achieve better results of our models. References "],
["examples.html", "Examples", " Examples "],
["story-covid19.html", "Chapter 11 Story covid19: eXplainable predictions for mortality 11.1 Figures and citations 11.2 Introduction 11.3 Model 11.4 Explanations 11.5 Summary and conclusions", " Chapter 11 Story covid19: eXplainable predictions for mortality Authors: Author 1 (University 1), Author 2 (University 2), Author 3 (University 3) Mentors: Mentor 1 (Affiliation 1), Mentor 2 (Affiliation 2) 11.1 Figures and citations Cite other materials this way: This book is created with (Xie 2015). Refer to figures or chatpers this way: In Figure 11.1 we show a CC BY-NA-SA logo. The next chapter is 7. FIGURE 11.1: Licence: Creative Commons Attribution NonCommercial ShareAlike 11.2 Introduction Put a description of the problem here. indicate the data source. Describe why this problem is important. Indicate the most important literature on the problem. 11.3 Model Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. 11.4 Explanations Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 11.5 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? References "],
["story-compas-recidivism-reloaded.html", "Chapter 12 Story COMPAS: recidivism reloaded 12.1 Introduction 12.2 Model 12.3 Explanations 12.4 Summary and conclusions", " Chapter 12 Story COMPAS: recidivism reloaded Authors: Author 1 (University 1), Author 2 (University 2), Author 3 (University 3) Mentors: Mentor 1 (Affiliation 1), Mentor 2 (Affiliation 2) 12.1 Introduction Put a description of the problem here. indicate the data source. Describe why this problem is important. Indicate the most important literature on the problem. 12.2 Model Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. 12.3 Explanations Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 12.4 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements This project is inspired by a fantastic book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich. We used the LIML project as cornerstone for this reopsitory. "],
["references.html", "References", " References Akshay Kumar, Rishabh Kumar. 2018. “Uplift Modeling : Predicting Incremental Gains.” 2018. http://cs229.stanford.edu/proj2018/report/296.pdf. Bates, Douglas, Martin Maechler, and Ben Bolker. 2020. Linear Mixed-Effects Models Using ’Eigen’ and S4. https://cran.r-project.org/web/packages/lme4/index.html. Biecek, Przemyslaw, and Tomasz Burzykowski. 2019. Explanatory Model Analysis. https://pbiecek.github.io/ema/. Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. Chen, Tianqi, and Carlos Guestrin. 2016. “XGBoost: A Scalable Tree Boosting System.” CoRR abs/1603.02754. http://arxiv.org/abs/1603.02754. Conway, Jennifer. 2018. “Artificial Intelligence and Machine Learning : Current Applications in Real Estate.” PhD thesis. https://dspace.mit.edu/bitstream/handle/1721.1/120609/1088413444-MIT.pdf. Din, Allan, Martin Hoesli, and Andre Bender. 2001. “Environmental Variables and Real Estate Prices.” Urban Studies 38: 1989–2000. Esmukov, Kostya. 2020. “Python Geocoding Toolbox.” https://geopy.readthedocs.io/en/latest/#. Gosiewska, Alicja, and Przemysław Biecek. 2019. “auditor: an R Package for Model-Agnostic Visual Validation and Diagnostics.” The R Journal 11 (2): 85–98. https://doi.org/10.32614/RJ-2019-036. Greenwell, B., B. &amp; Boehmke. 2019. Gbm: Generalized Boosted Regression Models. https://cran.r-project.org/web/packages/gbm/index.html. Guolin Ke, Thomas Finley, Qi Meng. 2017. “LightGBM: A Highly Efficient Gradient Boosting Decision Tree.” https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf. Heyman, Axel, and Dag Sommervoll. 2019. “House Prices and Relative Location.” Cities 95 (September): 102373. https://doi.org/10.1016/j.cities.2019.06.004. Hillstrom, Kevin. 2008. “The Minethatdata E-Mail Analytics and Data Mining Challenge Dataset.” 2008. https://blog.minethatdata.com/2008/03/minethatdata-e-mail-analytics-and-data.html. Jaroszewicz, S., and P. Rzepakowski. 2014. “Uplift Modeling with Survival Data.” In ACM Sigkdd Workshop on Health Informatics (Hi-Kdd’14). New York City, USA. Jaskowski, Maciej, and Szymon Jaroszewicz. 2012. “Uplift Modeling for Clinical Trial Data.” In. Kozak, Anna, and Przemyslaw Biecek. 2020. Local Variable Importance via Oscillations of Ceteris Paribus Profiles. https://cran.r-project.org/web/packages/vivo/index.html. Krzysztof, Rudaś, and Szymon Jaroszewicz. 2018. “Linear Regression for Uplift Modeling.” Data Min. Knowl. Discov. 32 (5): 1275–1305. Law, Stephen. 2017. “Defining Street-Based Local Area and Measuring Its Effect on House Price Using a Hedonic Price Approach: The Case Study of Metropolitan London.” Cities 60 (February): 166–79. https://doi.org/10.1016/j.cities.2016.08.008. Lee, Josh Xin Jie. 2018. “Simple Machine Learning Techniques to Improve Your Marketing Strategy: Demystifying Uplift Models.” 2018. https://medium.com/datadriveninvestor/simple-machine-learning-techniques-to-improve-your-marketing-strategy-demystifying-uplift-models-dc4fb3f927a2. Lundberg, Scott, and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In. Mostipak, Jesse. 2020. “Hotel Booking Demand.” https://www.kaggle.com/jessemostipak/hotel-booking-demand. Park, Byeonghwa, and Jae Bae. 2015. “Using machine learning algorithms for housing price prediction: The case of Fairfax County, Virginia housing data.” Expert Systems with Applications 42 (April). https://doi.org/10.1016/j.eswa.2014.11.040. “Pylift Package - Documentation.” n.d. https://pylift.readthedocs.io/en/latest/. R Core Team. 2018. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rzepakowski, Piotr, and Szymon Jaroszewicz. 2012. “Decision Trees for Uplift Modeling with Single and Multiple Treatments.” Knowledge and Information Systems - KAIS 32 (August). https://doi.org/10.1007/s10115-011-0434-0. scikit-learn. 2019a. GradientBoostingRegressor Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html. ———. 2019b. GridSearchCV Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html. ———. 2019c. OneHotEncoder Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html. ———. 2019d. StandardScaler Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html. Selim, H. 2009. “Determinants of House Prices in Turkey: Hedonic Regression Versus Artificial Neural Network.” Expert Systems with Applications 36: 2843–52. Sołtys, Michał, Szymon Jaroszewicz, and Piotr Rzepakowski. 2015. “Ensemble Methods for Uplift Modeling.” Data Mining and Knowledge Discovery 29 (November). https://doi.org/10.1007/s10618-014-0383-9. Therneau, B., T. &amp; Atkinson. 2019. Rpart: Recursive Partitioning and Regression Trees. https://cran.r-project.org/web/packages/rpart/index.html. Verbeke, W., and C. Bravo. 2017. Profit Driven Business Analytics: A Practitioner’s Guide to Transforming Big Data into Added Value. Wiley and Sas Business Series. Wiley. https://books.google.pl/books?id=NCA3DwAAQBAJ. Wright, Marvin N., and Andreas Ziegler. 2015. “Ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” https://doi.org/10.18637/jss.v077.i01. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. Zhao, Zhenyu, and Totte Harinen. 2019. “Uplift Modeling for Multiple Treatments with Cost Optimization.” http://arxiv.org/abs/1908.05372. "]
]
